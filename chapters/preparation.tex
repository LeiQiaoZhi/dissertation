\label{sec:Preparation}

Following the workflow of the app shown in Figure \ref{pipeline}, this chapter covers background materials related to \textit{procedural generation techniques} (Section \ref{Procedural Generation}) for creating implicit representations of scenes, and \textit{rendering methods} (Section \ref{Rendering}) for these implicit representations. Additionally, the chapter explores shaders and Graphics Processing Unit (GPU) architecture (Section \ref{GPU}), crucial elements since procedural generation and rendering are performed using fragment shaders.

The chapter then describes the requirements (Section \ref{sec:Requirement}) and software engineering techiniques (Section \ref{sec:Software Engineering}).


\myfigure{1.0}{pipeline}{H}
{shows the simplified workflow of the app. First parameters are tuned in the GUI and sent to the fragment shaders as uniforms. Then the procedural generation stage utilizes the parameters to create implicit representation of the scene, which is then rendered using ray marching.}


\section{Procedural Generation of Natural Environments}
\label{Procedural Generation}

This section provides background materials necessary for addressing the topic of procedurally generating and representing natural elements digitally. It starts by examining nature's properties and how Fractional Brownian Motion captures them, then moves on to discuss representing complex surfaces like terrains and clouds with implicit representations.

\subsection{Properties of Nature}

Understanding nature is the first step in simulating and procedurally generating it.

Nature, inherently random yet ordered, offers a complex canvas for study. In a forest, the placement of trees defies any rigid grid pattern; similarly, no two leaves are identical. Despite this randomness, nature isn't entirely without structure. Clouds, for instance, while seemingly chaotic, adhere to general shapes rather than resembling mere white noise. This blend of randomness with pattern is a hallmark of nature, exemplified by the principle of self-similarity. Self-similar objects in nature, whether they are clouds, mountain ranges, or tree barks, exhibit a fascinating characteristic: their parts mirror the whole in shape and texture. Even the minute lumps on a mountain's surface can strikingly resemble the mountain itself. Another observable property is the continuity and smoothness of natural changes, such as the gradual transition in soil salinity from a beach to inland areas.

To effectively procedurally generate nature, it's crucial to replicate this interplay of \textbf{self-similarity}, \textbf{continuity}, and \textbf{randomness}.

\subsection{Simulating the Properties of Nature}

To simulate these properties, we first consider classical Brownian motion before moving to a more general model, fractional Brownian motion (fBm), which embodies randomness, self-similarity, and continuity.

\subsubsection{Classical Brownian Motion}

Classical Brownian motion, denoted as $B(t)$, serves as an essential model in the study of stochastic processes. It describes the random movement of particles with several key properties:

\begin{enumerate}
    \item The process begins at a fixed point, typically zero, so $B(0) = 0$.
    \item  The increments of $B(t)$ over distinct, non-overlapping time intervals are independent. Specifically, for times $t_1 < t_2$ and $t_3 < t_4$ with $t_2 \leq t_3$, the increments $B(t_2) - B(t_1)$ and $B(t_4) - B(t_3)$ are independent of each other.
    \item The increment $B(t + \Delta t) - B(t)$ is normally distributed with a mean of 0 and a variance of $\Delta t$, described by the normal distribution $\mathcal{N}(0, \Delta t)$.
    \item The paths of $B(t)$ are continuous at all points but not differentiable at any point.
\end{enumerate}

Although classical Brownian motion encapsulates elements of randomness and continuity, it lacks self-similarity, a property crucial for mimicking the fractal-like structures observed in nature. 

\subsubsection{Fractal Brownian Motion}

Fractional Brownian motion (fBm) generalises the classical Brownian motion model. It incorporates long-range dependence or memory,  by introducing the Hurst exponent, $H$, which ranges between 0 and 1. 

The correlation of increments in fBm is directly influenced by the value of $H$. 
\begin{itemize}
    \item When $0.5 < H < 1$, the increments exhibit positive correlation, indicating a trend where increases or decreases in the process are likely to be followed by similar movements.
    \item When $0 < H < 0.5$, the increments display negative correlation.
    \item When $H=0.5$, fBm simplifies to classical Brownian motion, where increments are independent.
\end{itemize}

On key property of fBm is its self-similar structure, which means its statistical properties remain consistent under appropriate scaling of time and space. This property is a significant deviation from classical Brownian Motion and is vital for modeling phenomena that demonstrate fractal-like patterns. Akin to classical Brownian Motion, fBm maintains continuity in its paths.

Therefore, fBm has the desired properties of randomness, self-similarity and continuity. It is suitable for procedural generation of elements nature such as terrain and clouds.

\subsubsection{Constructing fBm with Fractal Noises}
\label{construct fbm}

\question{Shall I put this section in implementation or keep it here in preparation?}

A method for constructing fBm is to use fractal noises, specifically combining multiple layers of smooth gradient noise such as value noise.

\paragraph{Definition of layers}

The construction process starts by defining $n \in \mathbb{Z}^+$ layers of value noise, each with a specific frequency, amplitude, rotation, and translation. Value noise, as a smooth pseudo-random function, is described in Section \ref{value noise}.

The key parameters for layer $i$ are:

\begin{enumerate}
    \item \textbf{Frequency} ($f_i$): Controls the noise pattern's scale. Higher frequencies lead to more intricate patterns.
    \item \textbf{Amplitude} ($a_i$): Controls the impact of each layer on the overall noise. Typically, higher frequency layers have lower amplitudes.
    \item \textbf{Rotation}: Represented by a matrix $\mathbf{R_i}$.
    \item \textbf{Translation/Offset}: Indicated by a vector $\mathbf{o_i}$.
\end{enumerate}

\paragraph{Parameter adjustment}

The frequency and amplitude for each layer are defined using constant multiplicative factors, $\alpha$ and $\beta$, respectively. This allows for the simpler expression of the frequency and amplitude for all layers in terms of the initial values $f_0$ and $a_0$:

\begin{align}
f_i = \alpha^i \cdot f_0 \\
a_i = \beta^i \cdot a_0
\end{align}

Similarly, rotations and translations are defined using a consistent rotation matrix $\mathbf{R}$ and a translation vector $\mathbf{o}$:

\begin{align}
\mathbf{R_i} = \mathbf{R}^i \\
\mathbf{o_i} = \mathbf{o}^i
\end{align}

\paragraph{Summation of layers}

The final construction of fBm is achieved through a point-wise summation of these layers. Let $N_k(\mathbf{x})$ represent the value of the k-dimensional noise function at point $\mathbf{x}$. The fBm $\text{fBm}_k(\mathbf{x}, n)$ at point $\mathbf{x}$ is then given by:

\begin{equation}
    \text{fBm}_k(\mathbf{x}, n) = \sum_{i=0}^{n} a_i \cdot N_k(f_i \cdot \mathbf{R}_i\mathbf{x} + \mathbf{o}_i)
\end{equation}

Here, $n$ denotes the total number of layers. The sum of these adjusted layers results in a noise field that embodies the essential characteristics of fBm -- the smooth gradient noise creates randomness and continuity and layering of the noises with different frequencies creates self similarity.

\subsubsection{Value Noise}
\label{value noise}

\question{Shall I put this section in implementation or keep it here in preparation or maybe appendix?}

Value noise is a fundamental concept in the field of procedural generation, particularly in the construction of fBm (Section \ref{construct fbm}). This section provides a detailed explanation of value noise.

Let's denote the noise function as $N_k(\mathbf{x})$ for a point $\mathbf{x}$ in $k$-dimensional space. The function operates as follows:

\begin{enumerate}
    \item \textbf{Grid Definition}: Define a grid over the $k$-dimensional space where each point $\mathbf{p}_i$ on the grid has an associated random value $v_i$. The distribution of $v_i$is usually uniform.

    \item \textbf{Value Assignment}: Assign a random value $v_i$to each grid point $\mathbf{p}_i$. These values are generated using a pseudo-random number generator and remain constant for a given seed.

    \item \textbf{Interpolation}: For a point $\mathbf{x}$ not on the grid, identify the surrounding grid points $\{\mathbf{p_{i}\}}$ and interpolate the values $\{v_i\}$ at these points to calculate $N_k(\mathbf{x})$. The interpolation could be linear, cubic, or use other smoothing techniques, depending on the desired smoothness of the noise.
    
    \begin{equation}
        N(\mathbf{x}) = \text{Interpolate}(\{v_i\}, \{\mathbf{p}_i\}, \mathbf{x}) 
    \end{equation}
\end{enumerate}


\subsection{Implicit Representations}

How can large/infinite complex surfaces such as terrains and clouds be represented? This section focuses on implicit representation as a key solution, and introduces two special types of  implicit representations: heightmaps and Signed Distance Fields (SDFs).

\subsubsection{Implicit Representation: A Solution for Complex Surfaces}

In digital representation of 3D geometry, explicit methods like polygon meshes have traditionally been preferred, relying on vertices, edges, and faces to define surfaces. While well-supported in hardware and software, these methods face limitations with large or complex surfaces, leading to high storage and memory demands. Crucially, they also lack the ability to achieve infinite resolution, which is vital for rendering detailed and expansive landscapes.

In contrast, implicit representation is a powerful alternative. At its core, an implicit equation is a relation in the form of $R(x_1,\dots,x_n)=0$, where $R$ is the implicit function. The solution space of a n-dimensional implicit equation describes the surface of an n-dimensional shape. This allows complex surfaces such as large or even infinite terrains to be defined using a single function. This approach significantly reduces memory and storage requirements compared to traditional mesh-based methods. Additionally, the sign of the implicit function indicates whether the point is inside or outside the surface (negative when inside), simplifying spatial determinations crucial for advanced rendering techniques, such as ray marching.

\subsubsection{Heightmaps}
\label{Heightmap}

A heightmap $H(x,z)$ define the elevation $y$ of the terrain at each point $(x,z)$. This function is a scalar field, where each point in the field has a single value of elevation associated with it. The implicit equation representing terrain's geometry is expressed as:

\begin{equation}
y - H(x,z) = 0
\end{equation}

\todo{mention somewhere the coordinate system in this project has y pointing upwards}

\subsubsection{Signed Distance Functions}
\label{SDF}

Another potent implicit representation is the Signed Distance Function (SDF), symbolized as $S(x,y,z)$ in 3D space. For every point in space, the SDF assigns a value representing the shortest distance to the surface. Crucially, the sign of this value indicates whether the point is inside or outside the geometry. The implicit equation for an SDF-based terrain is simply:

\begin{equation}
S(x,y,z) = 0 
\end{equation}

It's important to note that heightmaps and SDFs are distinct; while heightmaps are 2D scalar fields providing elevation data, 3D SDFs are 3D scalar fields providing the shortest distance to the surface at each point. 


\section{Rendering}
\label{Rendering}

The rendering model of my project utilized ray marching as a per-pixel rendering technique. For every pixel on the screen, a ray, hereinafter referred to as the \textit{camera ray}, is emitted from the camera and directed towards the respective pixel. This process goes well naturally in the fragment shader, which can be run in parallel on the GPU.

Mathematically the camera ray can be expressed as:

\begin{equation}
 \mathbf{R}(t) = \mathbf{O} + t\mathbf{CR} 
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{O}$ is the ray origin, which is the camera position in the scene
    \item $\mathbf{CR}$ is the normalized direction vector pointing from the camera towards the pixel
    \item $t$ is the distance along the ray
\end{itemize}

For every pixel, the rendering process is:
\begin{enumerate}
    \item find the nearest intersection betwen the camera ray and scene objects
    \item apply textures and lighting based on information at the point of intersection to produce the color of the pixel
\end{enumerate}

Following this process, this section first introduces ray marching, which is employed to find intersections, and then describes some illumination adn shadow models used in this project.

\subsection{Ray Marching SDFs}
\label{Raymarching SDFs}

One of the most efficient techniques for rendering scenes composed of SDFs is an algorithm known as Sphere Tracing (\todo{reference}). In this context, consider a camera ray $\mathbf{R}(t)$, and the scene is assumed to contain multiple SDFs, denoted as $\text{SDF}_1, \dots, \text{SDF}_n$.

The core idea of Sphere Tracing is to iteratively move along the ray, incrementing the distance $t$ by the smallest distance to any surface in the scene. This distance is determined at each step as the minimum SDF value among all SDFs at the current point on the ray. The process is mathematically expressed as:

\begin{equation}
    t_{i+1} = t_{i} + \min_i(\text{SDF}_i(\mathbf{R}(t_i)))
\end{equation}

This process continues until the ray either exceeds a predefined maximum distance, $t_{max}$, or when a specified maximum number of steps is reached, suggesting no intersection within the scene's bounds. The process also terminates when the absolute value of the minimum SDF at the current ray position is less than a small threshold $\epsilon$, implying an intersection.

\begin{equation}
|\min_{i}(\text{SDF}_i(\mathbf{R}(t_i)))| < \epsilon 
\end{equation}

This concept is crucial in the rendering of trees, as discussed in a section 3.\todo{}. Figure \ref{raymarch_sdf} illustrates the Sphere Tracing algorithm in action, where it finds the intersection between the camera ray and two SDFs. 

\myfigure{0.7}{raymarch_sdf}{}
{Illustration of Sphere Tracing for Ray Marching SDFs. The camera emits a ray that is iteratively advanced by the minimum distance to the surfaces of SDF 1 and SDF 2. The ray progresses until it closely approaches the boundary of SDF 2, indicating an intersection.}

\subsection{Ray Marching Heightmaps}
\label{Raymarching Heightmaps}

When dealing with a single heightmap $H$, the Sphere Tracing algorithm is inapplicable since heightmaps are not SDFs. Here, a more rudimentary form of ray marching, termed the \textit{fixed-step ray marching algorithm}, is employed.

This algorithm advances the ray with a constant step size, $\lambda$.  The process is mathematically expressed as:

\begin{equation}
    t_{i+1} = t_{i} + \lambda
\end{equation}

At each step, the algorithm evaluates the implicit function $y-H(x,z)$. It terminates when this function's value drops below zero, indicating an intersection with the heightmap. To refine the exact point of intersection, linear interpolation is used between the last two points. The interpolation process is mathematically expressed as:

\begin{equation}
t^\prime = t - \lambda \frac{H(\mathbf{R}(t).xz) - \mathbf{R}(t).y}
    {H(\mathbf{R}(t).xz) - \mathbf{R}(t).y - H(\mathbf{R}(t - \lambda).xz) - \mathbf{R}(t - \lambda).y}
\end{equation}

where:
\begin{itemize}
    \item $t$ is the distance when the camera ray first went below the heightmap.
    \item $t^\prime$ is the distance after interpolation.
\end{itemize}

This dissertation further explores numerous optimizations of this algorithm in section \ref{Terrain Raymarching}. Figure \ref{raymarch_terrain} illustrates the fixed-step ray marching algorithm in action, where it finds the intersection between the camera ray and a 1D heightmap.

\myfigure{0.7}{raymarch_terrain}{}
{Illustration of Fixed-step Ray Marching for Heightmaps. The camera emits a ray that is iteratively advanced by a constant distance to the surface of a terrain. The ray progresses until it goes below the terrain, followed by a linear interpolation.}


\subsection{Illumination and Shadows}

This section outlines the process for computing the color of a pixel once an intersection with a scene object is identified. The process applies textures and calculates lighting to determine the final color for each pixel.

In this project, the material colors are determined procedurally, which is be elaborated upon in \ref{Terrain Procedural Texturing}.

The lighting aspect is explored through the Phong reflection model, considering Fresnel reflectance and shadows. Given the project's scope, a simplified lighting model suffices, ensuring clairty and efficiency.

\subsubsection{Phong Reflection Model}
\label{Phong}

The Phong Model is an empirical shading model. It comprises three primary components that collectively determine the color of a pixel of a surface,  based on its material properties and light interactions.

\paragraph{Ambient Reflection} This represents the base level of light present in the scene, irrespective of direct light sources. It's used to simulate the effect of scattered light in the environment. Mathematically, it's a constant term:

\begin{equation}
\label{ambient}
k_a \cdot I
\end{equation}

with $k_a$ being the ambient reflectivity of the surface and $I$ the intensity of ambient light.

\paragraph{Diffuse Reflection} This accounts for the direct light scattering that happens when light hits rough surfaces. This scattering causes the light to reflect evenly in all directions. It's calculated using Lambert’s cosine law: 
    
\begin{equation}
\label{diffuse}
I_d = k_d \cdot (L \cdot N) \cdot I
\end{equation}

where $k_d$ is the diffuse reflectivity, $L$ is the normalized light vector, $N$ is the normalized surface normal, and $I$ is the light's intensity.
    
\paragraph{Specular Reflection} This simulates the bright spots of light that appear when light is reflected at specific angles, like on shiny surfaces. It is determined using the formula: 

\begin{equation}
\label{specular}
I_s = k_s \cdot (R \cdot V)^n \cdot I
\end{equation}

where $k_s$ is the specular reflectivity, $R$ is the reflection vector, $V$ is the view vector, and $n$ is the shininess coefficient which controls the size of the specular highlight.

\subsubsection{Fresnel Reflectance}
\label{Fresnel}

The Fresnel Reflection principle describes how the amount of light reflected by a surface varies with the angle of incidence. 

The Fresnel equations provide a way to calculate the reflection coefficient, which determines the ratio of reflected light at different angles of incidence. In its simplest form, applicable to non-conducting materials, the Fresnel reflectance at normal incidence can be represented as:

\begin{equation}
F = \left( \frac{n_1 - n_2}{n_1 + n_2} \right)^2
\end{equation}

where $n_1$ and  $n_2$ are the refractive indices of the medium from which the light is coming and the medium it is entering, respectively.

In computer graphics, a common approximation of the Fresnel Effect is the Schlick's approximation, given by:

\begin{equation}
\label{schlick}
F(\theta) = F_0 + (1 - F_0)(1 - \cos(\theta))^5
\end{equation}

where $F_0$  is the reflectance at normal incidence (calculated from the indices of refraction), and  $\theta$  is the angle between the view direction and the normal to the surface.

Incorporating Fresnel Reflection into rendering algorithms significantly enhances the realism of materials, especially noticable in materials like water. 

\subsubsection{Shadows}
\label{Shadows}

\paragraph{Hard Shadows}
The simplest shadow calculation is the binary determination of whether light reaches a point or not, resulting in hard shadows with distinct, sharp edges. In ray marching, a shadow ray is cast from the point of intersection towards the light source, and if any object is encountered, the point is deemed to be in shadow.

\paragraph{Soft Shadows}
In reality, shadows often exhibit soft edges due to the extended size of light sources, like the Sun. Soft shadows display an \textbf{umbra}, the darker core where light is completely blocked, and a \textbf{penumbra}, the lighter area where the light is partially obscured.

One commonly used method involves modifying the shadow ray's contribution based on the minimum distance to the scene geometry along its path. Here's a standard approach (\todo{cite}):

\begin{enumerate}
    \item Cast a shadow ray from the intersection point towards the light source. During ray marching, maintain a record of the minimum distance, denoted as $d_{\text{min}}$ , from the ray to the scene geometry at each step. Moreover, the process terminates immediately if an intersection is found ($d_{\text{min}}\le 0$), indicating hard shadow.

    \item Calculate a penumbra factor $P$ based on $d_{\text{min}}$. This factor is used to soften the shadow's edge. As the shadow ray remains distant from surfaces, the penumbra factor increases, softening the shadow. A typical calculation for $P$ might be:

    \begin{equation}
       P = \exp(-k \cdot d_{\text{min}})
    \end{equation}

    where $k$ is a constant that controls the softness of the shadow's edge. A larger value of $k$ results in softer shadows.

    \item Determine the shadow intensity $S$ at the point. In the simplest form, it can be linearly related to the penumbra factor:

    \begin{equation}
    S = \min(1, P)
    \end{equation}

    This equation ensures that the shadow intensity smoothly transitions from fully shadowed (0) to fully lit (1).
\end{enumerate}

Figure \ref{shadow} showcases the soft shadow calculations for two intersections in an example scene, where one intersection lies in the penumbra region, and the other lies in the umbra region.

\myfigure{0.5}{shadow}{}
{Illustration of soft shadow calculations in a simple example scene. For the point in the penumbra region, its shadow ray doesn't intersect with any scene objects. The minimum distance $d_{\text{min}}$ is incurred at $P_2$. For the point in the umbra region, its shadow ray intersects with a scene object.}


\section{Shader Execution}
\label{GPU}

Unlike traditional programming executed on the Central Processing Unit (CPU), shader programs are executed on the Graphics Processing Unit (GPU). This execution model leverages the GPU’s architecture, which is designed for parallel processing, featuring hundreds or thousands of small processing cores. These cores are grouped into larger units, often called streaming multiprocessors (SMs) or compute units (CUs). The architecture is detailed in \ref{gpu_architecture}.

In graphics pipelines, there are several types of shaders, each serving a specific purpose:
\begin{itemize}
    \item \textbf{Vertex Shaders}: Process vertex data and determine vertex positions in screen space.
    \item \textbf{Fragment (or Pixel) Shaders}: Calculate the color and other attributes of each pixel.
    \item \textbf{Geometry Shaders}: Generate geometry from vertices.
\end{itemize}

In this project, the majority of the procedural generation and rendering processes are implemented in \textbf{fragment shaders}.

\paragraph{Workload distribution}
When a scene is being rendered, it's divided into fragments. The GPU's control unit assigns groups of these fragments (often in a grid pattern, such as 2x2 "pixel-quads") to the different processing cores. This distribution ensures that the workload is balanced across the GPU's cores. This process is detailed in Figure \ref{gpu_architecture}.

\paragraph{Efficient parallelisation}
Many GPU architectures use a SIMD(Single Instruction, Multiple Data) approach. In this model, each core executes the same instruction at the same time but on different data (fragments). This is efficient for fragment shaders because each fragment often undergoes similar processing.

\myfigure{1.0}{gpu_architecture}{}
{Schematic of GPU parallel processing architecture. Fragments are organized into pixel quads, and the Control Unit dispatches tasks for executing fragment shaders to multiple Streaming Multiprocessors and their respective cores.}

\section{Requirement Analysis}
\label{sec:Requirement}

The project’s success criteria (Appendix \todo{ref}) forms the requirements:

\begin{itemize}
    \item Achieve real-time raymarched rendering of natural terrains, encompassing shadows, shading, and colouring. Submit screen recordings showcasing the rendered outcomes to highlight the real-time performance capabilities.
    \item Attain realistic detailing in the terrains, including features like foliage and rocks, complemented by an authentic backdrop of clouds and sky.
    \item Develop a desktop application that offers camera controls, allows real-time hyperparameter adjustments, and supports saving and loading of hyperparameters in an accessible plain-text format.
\end{itemize}

\subsection{MoSCow Analysis}

I then conducted MoSCow analysis to refine the requirements into deliverables and give priorities to the deliverables, as detailed in Table \ref{deliverable_table}. Core deliverables are in the Must-Have category, while extensions are in the Should-Have or Could-Have cateogries. 

\figureastable{1.0}{deliverable_table}{}
{lists all project deliverables. The deliverables are split into 4 types, based on the success criteria.}

\section{Software Engineering Tools and Techniques}
\label{sec:Software Engineering}

\paragraph{Development model}
I adopted the Agile methodology, with its emphasis on iterative and incremental development. I broke each deliverable down into smaller tasks, and managed them in a Kanban board, organized by stages of development. This not only facilitated better organization but also allowed for continuous delivery and ongoing improvements based on feedback and experimental results.

\paragraph{Libraries and language}
I chose OpenGL as the graphics interface due to its relative ease of setup. Since the majority of the project is implemented in fragment shaders, I did not need to extensively modify the rendering pipeline. GLSL, integral to OpenGL, was the choice of the shader language. Its C-like syntax and comprehensive documentation made it easy for me to pick up and start working with shaders.

\paragraph{Testing strategy}
The project's extensive use of GLSL (OpenGL Shading Language) brought unique challenges, notably the scarcity of testing frameworks, debuggers and general functionalities. To counter this, I employed various methods to aid in testing and debugging. By incorporating color-coding techniques akin to heat maps, I could visualize quantities such as depth and normals. Additionally, I created visual "gizmos" like lines and circles to aid in the debugging process. The app's user interface allowed seamlessly switching between different rendering targets, enabling the display of diverse debugging visuals on demand. I made sure to test each individual feature thoroughly this way before integration with other features.

\paragraph{Import in GLSL}
To address the lack of import functionality in GLSL, I wrote a custom preprocessor in C++. This allowed me to maintain modular shader files, preventing the codebase from becoming an unwieldy monolith. I included a shader dependency graph in Figure \ref{dependency} to demonstrate the organization of the shader files.

\paragraph{Code styling}
I paid careful attention to naming conventions across the C++ and GLSL codebases, opting for snake case for functions and variables, and Pascal case for classes and namespaces. This consistency, coupled with specific prefixes and suffixes for variable types (such as "in" and "out" parameters in GLSL), enhanced code readability and maintainability. Considering the limited GLSL syntax highlighting in IDEs, I leveraged these naming conventions to implement custom syntax highlighting using regular expressions.

\paragraph{Version control and backup}
I utilized Git for version control and GitHub as a remote repository for backup. I made sure to make regular commits and pushes.

\myfigure{1.0}{dependency}{}
{dependency graph for all the shader files in the project. A $\rightarrow$ B means A imports B.}

\section{Software License}
\label{sec:License}

Table \ref{tools_table} catalogs the licenses for all the tools and libraries utilized in this project.

The MIT license was chosen for this project due to its simplicity, permissiveness and compatibility. It allows others to freely use, modify, and distribute the software while providing protection from liability.


\section{Starting Point}
\label{sec:Starting Point}

\paragraph{Libaries and tools}
My project utilized a selection of pre-existing libraries, as detailed in Table \ref{tools_table}. Aside from these libraries, the project was developed from scratch, with particular attention given to the GLSL fragment shaders I wrote, which did not utilize any external libraries.

\paragraph{Prior experience}
I had prior experience with OpenGL wrapper libraries, gained through the "Introduction to Graphics" course and a past internship, although my direct experience with OpenGL is limited. I had worked with C++, Dear ImGui and CMake in a previous summer internship. While I had some exposure to HLSL through "Introduction to Graphics", my experience with GLSL prior to this project was nonexistent.

\figureastable{1.0}{tools_table}{}
{Overview of purposes and licenses for utilized tools and libraries in the project.}