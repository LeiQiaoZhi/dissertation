\label{sec:Preparation}

Following the workflow of the app shown in Figure \ref{pipeline}, this chapter covers background materials related to \textit{procedural generation techniques} (Section \ref{Procedural Generation}) for creating implicit representations of scenes, and \textit{rendering methods} (Section \ref{Rendering}) for these implicit representations. Additionally, the chapter describes shaders and Graphics Processing Unit (GPU) architecture (Section \ref{GPU}), crucial elements since procedural generation and rendering are performed in fragment shaders.

The chapter then describes the requirements (Section \ref{sec:Requirement}) and software engineering techniques (Section \ref{sec:Software Engineering}).


\myfigure{1.0}{pipeline}{H}
{
% DONE
% \JM{All non-rendered output figures should be stored in vector graphics formats e.g PDF or SVG.}
% \todo{make every diagram svg}
Illustrating the app's workflow: Parameters are adjusted in the GUI and passed to fragment shaders as uniforms. Next, these parameters drive the procedural generation stage, creating the scene's implicit representation, which is rendered through ray marching.}


\section{Procedural Generation of Natural Environments}
\label{Procedural Generation}

This section provides background materials necessary for addressing the topic of procedurally generating and representing natural elements digitally. It starts by examining nature's properties and how fractional Brownian motion (fBm) captures them, then discusses representing complex surfaces like terrains and clouds with implicit representations.

\subsection{Properties of Nature}
\label{Properties of Nature}

Understanding nature is the first step in simulating and procedurally generating it.

Nature, inherently random yet ordered, offers a complex study canvas. In forests, tree placement does not follow rigid patterns, and no two leaves are identical. Yet nature maintains structure: clouds, though chaotic, follow familiar shapes rather than resembling pure noise. This blend of randomness and pattern is central to nature, often exemplified by self-similarity. Clouds, mountains, and tree bark all show this feature, where parts mimic the whole in shape and texture. Additionally, natural transitions, like the gradual shift in soil salinity from the beach to inland, reveal the continuity and smoothness of change.

To effectively procedurally generate nature, it's crucial to replicate this interplay of \textbf{randomness}, \textbf{self-similarity}, and \textbf{continuity}.

\subsection{fBm Captures the Properties of Nature}
\label{FBM}

To model the properties of natural elements, I chose \textbf{fBm} in this project. fBm extends classical Brownian motion, introducing long-range dependence through the Hurst exponent \cite{mandelbrot_fractional_1968}. fBm is characterized by self-similarity \cite{dieker_simulation_2004}, meaning its statistical properties are consistent across different scales. Similar to classical Brownian Motion, fBm's paths are continuous and integrate randomness through Gaussian distribution \cite{mandelbrot_fractional_1968}. These properties make fBm suitable for procedural generation of natural elements such as terrain and clouds.

To construct fBm, I employed fractal noises \cite{vivo_fractal_nodate}, specifically combining multiple layers of smooth gradient noise such as value noise (Section \ref{value noise}).

First, define $n \in \mathbb{Z}^+$ layers of value noise, each with a specific frequency ($f_i$), amplitude ($a_i$), rotation ($\mathbf{R_i}$), and translation ($\mathbf{o_i}$), defined as:

\begin{align}
f_i &= \alpha^i \cdot f_0 \\
a_{i} &= \beta^i \cdot a_{0}\\
\mathbf{R_i} &= \mathbf{R}^i \\
\mathbf{o_i} &= i\cdot\mathbf{o}
\end{align}

Finally, do a point-wise summation of these layers. Let $N_k(\mathbf{P})$ represent the value of the k-dimensional noise function at point $\mathbf{P}$. A n-layer fBm at point $\mathbf{P}$ is then given by:

\begin{equation}
    \text{fBm}_k(\mathbf{P}) = \sum_{i=0}^{n} a_i \cdot N_k(f_i \cdot \mathbf{R}_i\mathbf{P} + \mathbf{o}_i)
\end{equation}

This result embodies the essential characteristics of fBm -- the smooth gradient noise provides randomness and continuity, and layering noises with different frequencies creates self similarity.


\subsection{Implicit Representations}

How can large/infinite complex surfaces such as terrains and clouds be represented? This section focuses on implicit representation as a key solution, and describes two special types of  implicit representations: heightmaps and Signed Distance Fields (SDFs).

% \JM{'Introduces' is too strong of a word here; it is usually applied when a novel concept is proposed for the first time.}

\subsubsection{Implicit Representation: A Solution for Complex Surfaces}

In digital representation of 3D geometry, explicit representations like polygon meshes have traditionally been preferred, relying on vertices, edges, and faces to define surfaces. While well-supported in hardware and software, these methods face limitations with large or complex surfaces, leading to high storage and memory demands \cite{team_main_2019}. Crucially, they also lack the ability to achieve infinite resolution \cite{team_main_2019}, which is vital for rendering detailed and expansive landscapes.

In contrast, implicit surface is a powerful alternative. At its core, an implicit equation is a relation in the form of $R(x_1,\dots,x_n)=0$, where $R$ is the implicit function \cite{osher_implicit_2006}. The solution space of a n-dimensional implicit equation describes the surface of an n-dimensional shape. This allows complex surfaces such as large or even infinite terrains to be defined using a single function. This approach significantly reduces memory and storage requirements compared to traditional mesh-based methods. Additionally, the sign of the implicit function indicates whether the point is inside or outside the surface (negative when inside) \cite{osher_implicit_2006}, which is convenient for ray marching.

\subsubsection{Heightmaps}
\label{Heightmap}

A heightmap $H(x,z)$ define the elevation $y$ of the terrain at each point $(x,z)$. The implicit equation representing terrain's surface is expressed as:

\begin{equation}
y - H(x,z) = 0
\end{equation}

\subsubsection{Signed Distance Functions}
\label{SDF}

Another implicit representation is the Signed Distance Function (SDF) \cite{osher_signed_2006}. For each point in space, the SDF assigns a value indicating the shortest distance to the surface. The sign of this value determines if the point lies inside or outside the geometry. The implicit equation for an SDF-based surface is:

\begin{equation}
S(x,y,z) = 0 
\end{equation}

% It's important to note that heightmaps and SDFs are distinct; while heightmaps are 2D scalar fields providing elevation data, 3D SDFs are 3D scalar fields providing the shortest distance to the surface at each point. 


\section{Rendering}
\label{Rendering}

The rendering model of my project utilized ray marching as a per-pixel rendering technique. For every pixel on the screen, a ray, termed as the \textbf{camera ray}, is emitted from the camera and directed towards the respective pixel. This process goes well naturally in the fragment shader, which can be run in parallel on the GPU.

Mathematically the camera ray can be expressed as:

\begin{equation}
 \mathbf{R}(t) = \mathbf{C} + t\cdot\mathbf{r_c} 
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{C}$ is the world coordinates of the camera.
    \item $\mathbf{r_c}$ is the normalized direction vector pointing from the camera towards the pixel.
    % \todo{change the notations globally}
    % \JM{Use individual letters for symbols} 
    \item $t$ is the distance along the ray.
\end{itemize}

For every pixel, the rendering process is:
\begin{enumerate}
    \item find the nearest intersection between the camera ray and scene objects.
    \item apply textures and lighting based on information at the point of intersection to produce the color of the pixel.
\end{enumerate}

Following this process, this section first introduces ray marching, which is employed to find intersections, and then describes some illumination and shadow models used in this project.

\subsection{Ray Marching SDFs}
\label{Raymarching SDFs}

One of the most efficient techniques for rendering scenes composed of SDFs is an algorithm known as \textbf{Sphere Tracing} \cite{hart_sphere_1995}. In this context, consider a camera ray $\mathbf{R}(t)$, and the scene is assumed to contain multiple SDFs, denoted as $\text{SDF}_1, \dots, \text{SDF}_n$.

\mywrapfigure{0.4}{raymarch_sdf}{r}
{Illustration of Sphere Tracing for Ray Marching SDFs. 
% The camera emits a ray that is iteratively advanced by the minimum distance to the surfaces of SDF 1 and SDF 2.
The ray progresses until it closely approaches the boundary of SDF 2, indicating an intersection. }{}
% \JM{This figure should be smaller.} \todo{make smaller and inline on the right}

The core idea of Sphere Tracing is to iteratively move along the ray, incrementing the distance $t$ by the smallest distance to any surface in the scene, as illustrated in Figure \ref{raymarch_sdf}. The process is expressed as:

\begin{equation}
    t_{i+1} = t_{i} + \min_i(\text{SDF}_i(\mathbf{R}(t_i)))
\end{equation}

This process continues until the ray either exceeds a predefined maximum distance, $t_{max}$, or when a specified maximum number of steps is reached, suggesting no intersection within the scene's bounds. The process also terminates when the absolute value of the minimum SDF at the current ray position is less than a small threshold $\epsilon$, implying an intersection:

\begin{equation}
|\min_{i}(\text{SDF}_i(\mathbf{R}(t_i)))| < \epsilon 
\end{equation}

This concept is crucial in the rendering of trees, as discussed in Section \ref{Tree Ray Marching}.



\subsection{Ray Marching Heightmaps}
\label{Raymarching Heightmaps}

When dealing with a heightmap $H$, the Sphere Tracing algorithm is inapplicable since heightmaps are not SDFs. Here, a more rudimentary form of ray marching, termed the \textbf{fixed-step ray marching algorithm} \cite{quilez_raymarching_nodate}, is employed.

% \mywrapfigure{0.45}{raymarch_terrain}{r}
% {Illustration of Fixed-step Ray Marching for Heightmaps. 
% }{}


This algorithm advances the ray with a constant step size, $\lambda$:

\begin{equation}
    t_{i+1} = t_{i} + \lambda
\end{equation}

\minipagewrap{0.56}{raymarch_terrain}{}
{Illustration of Fixed-step Ray Marching for Heightmaps.}{
At each step, the algorithm evaluates the implicit function $y-H(x,z)$. It terminates when this function's value drops below zero, indicating an intersection with the heightmap. To refine the exact point of intersection, linear interpolation is used between the last two points, expressed as:
}
\vspace{\baselineskip}

\begin{equation}
t^\prime = t - \lambda \frac{H(\mathbf{R}(t).xz) - \mathbf{R}(t).y}
    {H(\mathbf{R}(t).xz) - \mathbf{R}(t).y - H(\mathbf{R}(t - \lambda).xz) - \mathbf{R}(t - \lambda).y}
\end{equation}

Here, $t$ is the distance when the camera ray first went below the heightmap, and $t^\prime$ is the distance after interpolation.

This dissertation further explores several optimizations of this algorithm in Section \ref{Terrain Raymarching}. Figure \ref{raymarch_terrain} illustrates the fixed-step ray marching algorithm in action, where it finds the intersection between the camera ray and a 1D heightmap.




\subsection{Illumination and Shadows}

This section outlines the process for computing the color of a pixel once an intersection with a scene object is identified. The process applies textures and calculates lighting to determine the final color for each pixel.

In this project, the material colors are determined procedurally, which is detailed in \ref{Terrain Procedural Texturing}. Given the project's scope and focus, a simplified lighting model suffices. Therefore, I used the Phong reflection model \cite{phong_illumination_1975}, together with Fresnel reflectance and soft shadows.

% \subsubsection{Phong Reflection Model}
% \label{Phong}

% The Phong Model is an empirical shading model. It comprises three primary components that collectively determine the color of a pixel of a surface,  based on its material properties and light interactions.

% \paragraph{Ambient Reflection} This represents the base level of light present in the scene, irrespective of direct light sources. It's used to simulate the effect of scattered light in the environment. Mathematically, it's a constant term:

% \begin{equation}
% \label{ambient}
% k_a \cdot I
% \end{equation}

% with $k_a$ being the ambient reflectivity of the surface and $I$ the intensity of ambient light.

% \paragraph{Diffuse Reflection} This accounts for the direct light scattering that happens when light hits rough surfaces. This scattering causes the light to reflect evenly in all directions. It's calculated using Lambert’s cosine law \todo{CITE}: 
    
% \begin{equation}
% \label{diffuse}
% I_d = k_d \cdot (\mathbf{l} \cdot \mathbf{n}) \cdot I
% \end{equation}

% where $k_d$ is the diffuse reflectivity, $\mathbf{l}$ is the normalized light vector, $\mathbf{n}$ is the normalized surface normal, and $I$ is the light's intensity.
    
% \paragraph{Specular Reflection} This simulates the bright spots of light that appear when light is reflected at specific angles, like on shiny surfaces. It is determined using the formula: 

% \begin{equation}
% \label{specular}
% I_s = k_s \cdot (\mathbf{r} \cdot \mathbf{v})^n \cdot I
% \end{equation}

% where $k_s$ is the specular reflectivity, $\mathbf{r}$ is the reflection vector, $\mathbf{v}$ is the view vector, and $n$ is the shininess coefficient which controls the size of the specular highlight.

\subsubsection{Fresnel Reflectance}
\label{Fresnel}

The Fresnel Reflection principle describes how the amount of light reflected by a surface varies with the angle of incidence \cite{iten_understanding_2018}. 

The Fresnel equations provide a way to calculate the reflection coefficient, which determines the ratio of reflected light at different angles of incidence. Using the Schlick's approximation \cite{dunn_schlicks_nodate}, the Fresnel reflectance at normal incidence, $F_0$, and at incident angle $\theta$, $F(\theta)$, are given by:
% In its simplest form, applicable to non-conducting materials, the Fresnel reflectance at normal incidence can be represented as:

\begin{equation}
F_0 = \left( \frac{n_1 - n_2}{n_1 + n_2} \right)^2
\end{equation}

and


% In computer graphics, a common approximation of the Fresnel Effect is the Schlick's approximation \cite{dunn_schlicks_nodate}, given by:

\begin{equation}
\label{schlick}
F(\theta) = F_0 + (1 - F_0)(1 - \cos(\theta))^5
\end{equation}

Here, $n_1$ and  $n_2$ are the refractive indices of the medium from which the light is coming and the medium it is entering, respectively.

Incorporating Fresnel Reflection into rendering algorithms enhances the realism of materials, especially noticeable in materials like water. 

\subsubsection{Shadows}
\label{Shadows}

\mywrapfigure{0.35}{shadow}{r}
{Illustration of soft shadow calculations. For the point in penumbra, its shadow ray doesn't intersect with any scene objects, with $d_{\text{min}}$ incurred at $\mathbf{P}_2$.}{16}


\paragraph{Hard Shadows}
The simplest shadow calculation is the binary determination of whether light reaches a point or not, resulting in hard shadows with distinct, sharp edges. In ray marching, a shadow ray is cast from the point of intersection towards the light source, and if any object is encountered, the point is deemed to be in shadow.

\paragraph{Soft Shadows}

In the physical world, shadow edges are often soft due to the finite size of light sources like the Sun, resulting in shadows with an \textbf{umbra}—the fully shadowed region—and a \textbf{penumbra}—the partially illuminated area.

A technique to simulate this involves adjusting the shadow contribution based on the nearest distance to the geometry encountered by a shadow ray, $d_{\text{min}}$. Ray marching can approximate this distance, as demonstrated in Figure \ref{shadow}, with a more detailed explanation in Section \ref{Terrain Shadows} and \ref{Tree Shadows}.

% \question{Shall I leave the method below for implementation? It seems redundant to reiterate variations of this algorithm in shadows for terrain and trees.}
% \JM{Yes, and it can likely be shortened quite a bit }
% \todo{trim to a simple text description, and ref implementations}

% Figure \ref{shadow} showcases the soft shadow calculations for two intersections in an example scene, where one intersection lies in the penumbra region, and the other lies in the umbra region.

% \JM{This figure can be made smaller, try to match the text size to that of the document}
% \todo{make figure smaller and inline on the right}
% }


\section{Shader Execution}
\label{GPU}

Unlike traditional programming executed on the Central Processing Unit (CPU), shader programs are executed on the Graphics Processing Unit (GPU). This execution model leverages the GPU’s architecture, which is designed for parallel processing, featuring hundreds or thousands of small processing cores. These cores are grouped into larger units, often called streaming multiprocessors (SMs) or compute units (CUs), as illustrated in Figure \ref{gpu_architecture}.

In graphics pipelines, there are several types of shaders:
\begin{itemize}
    \item \textbf{Vertex Shaders}: Process vertex data and determine vertex positions in screen space.
    \item \textbf{Fragment (or Pixel) Shaders}: Calculate the color and other attributes of each pixel.
    \item Geometry shaders and compute shaders are not used in this project.
\end{itemize}

In this project, the procedural generation and rendering processes are implemented in \textbf{fragment shaders}.

\paragraph{Workload Distribution}
When a scene is being rendered, it is divided into fragments. The GPU's control unit assigns groups of these fragments (often in a grid pattern, such as 2x2 ``pixel-quads") to the different processing cores. This distribution ensures that the workload is balanced across the GPU's cores, as illustrated in Figure \ref{gpu_architecture}.

\paragraph{Efficient Parallelisation}
Many GPU architectures use a SIMD (Single Instruction, Multiple Data) approach. In this model, each core executes the same instruction at the same time but on different data (fragments). This is efficient for fragment shaders because each fragment often undergoes similar processing.

\myfigure{0.8}{gpu_architecture}{}
{Schematic of GPU parallel processing architecture. Fragments are organized into pixel quads, and the Control Unit dispatches tasks for executing fragment shaders to multiple Streaming Multiprocessors and their respective cores.}

\section{Requirement Analysis}
\label{sec:Requirement}

\subsection{Success Criteria}
The project’s success criteria (Appendix \todo{ref}) forms the requirements:

\begin{itemize}
    \item Achieve real-time ray marched rendering of procedurally generated natural terrains, encompassing shadows, shading, and coloring. 
    \item Attain realistic detailing in the terrains, complemented by an authentic backdrop of clouds and sky.
    \item Develop a desktop application that offers camera controls, allows real-time parameter adjustments, and supports saving and loading of parameters.
\end{itemize}

\subsection{MoSCow Analysis}

I then conducted \textbf{MoSCow analysis} \cite{clegg_case_1994} (Must have, Should have, Could have, Won't have this time) to refine the requirements into deliverables and give priorities to the deliverables, as detailed in Table \ref{deliverable_table}. Core deliverables are in the Must-Have category, while extensions are in the Should-Have or Could-Have categories. 

\figureastable{1.0}{deliverable_table}{h!}
{Outlining project deliverables, categorized into four types according to the success criteria.}

\section{Software Engineering Tools and Techniques}
\label{sec:Software Engineering}

\paragraph{Development Model}
I adopted the Agile methodology, emphasizing iterative and incremental development. I broke down each deliverable into smaller tasks and managed them on a Kanban board, organized by development stages. Working in two-week sprints allowed me to ensure consistent progress while remaining flexible in prioritizing important tasks.

% \question{Shall I include a screenshot for the Kanban board?}

\paragraph{Libraries and Language}
My application was implemented in C++, a popular industry-standard language for computer graphics projects. For shader programming, I chose GLSL (OpenGL Shading Language), integral to OpenGL. Its C-like syntax and comprehensive documentation made it easy to learn and use for shader development. A list of the libraries used is provided in Table \ref{tools_table}


\paragraph{Testing Strategy}
The project's extensive use of GLSL brought unique challenges, notably the scarcity of testing frameworks, debuggers and general functionalities. To counter this, I employed various methods to aid in testing and debugging, detailed in Section \ref{Visual Debugging}.
% By incorporating color-coding techniques akin to heat maps, I could visualize quantities such as depth and normals. Additionally, I rendered visual ``gizmos" like lines and circles in shaders to aid in the debugging process. The app's user interface allowed seamlessly switching between different rendering targets, enabling the display of diverse debugging visuals on demand. I made sure to test each individual feature thoroughly this way before integration with other features.

\paragraph{Import in GLSL}
To address the lack of import functionality in GLSL, I wrote a custom preprocessor in C++. This allowed me to maintain modular shader files, preventing the codebase from becoming an unwieldy monolith. I included a shader dependency graph in Figure \ref{dependency} to demonstrate the organization of the shader files.

\paragraph{Code Styling}
I paid careful attention to naming conventions across the C++ and GLSL codebases, enhanced code readability and maintainability. Considering the limited GLSL syntax highlighting in IDEs, I leveraged these naming conventions to implement custom syntax highlighting using regular expressions.

\paragraph{Version Control and Backup}
For both the code repository and the dissertation write-up, I utilized Git for version control and GitHub as a remote repository for backup. I made sure to make regular commits and pushes.

\myfigure{1.0}{dependency}{}
{Dependency graph of shader files in the project, where A $\rightarrow$ B denotes ``A imports B".}

\section{Software License}
\label{sec:License}

Table \ref{tools_table} catalogs the licenses for all the tools and libraries utilized in this project.

The \textbf{MIT license} was chosen for this project due to its simplicity, permissiveness and compatibility. It allows others to freely use, modify, and distribute the software while providing protection from liability.


\section{Starting Point}
\label{sec:Starting Point}

\paragraph{Libraries and Tools}
My project utilized a selection of pre-existing libraries, as detailed in Table \ref{tools_table}. Aside from these libraries, the project was developed from scratch. 

\paragraph{Prior Experience}
I had prior experience with OpenGL wrapper libraries, gained through the ``Introduction to Graphics" course and a past internship, although my direct experience with OpenGL is limited. I had worked with C++, Dear ImGui and CMake in a previous internship. While I had some exposure to HLSL through ``Introduction to Graphics", my experience with GLSL prior to this project was minimal.

\figureastable{1.0}{tools_table}{}
{Overview of purposes and licenses for utilized tools and libraries in the project.}