\label{sec:3}

The app's workflow, as shown in Figure \ref{pipeline}, consists of three main stages: user interface (UI), procedural generation, and rendering. Given the project's scope -- encompassing the procedural generation and rendering of multiple natural elements -- it is practical to discuss each element individually, treating UI as a distinct subject.

The chapter begins with an overview of the implementation, followed by detailed sections on each natural element: terrain, trees, water, the atmosphere and clouds. Finally, it discusses procedural planets, input handling, and the UI.

\section{Implementation Overview}
\label{Implementation Overview}

This section outlines the core architecture of the app, focusing on the execution sequences of both the graphics pipeline and the fragment shader within that pipeline.

\subsection{Pipeline Execution Order}
\label{Pipeline Execution Order}

\mywrapfigure{0.6}{pipeline_order}{r}
{Outlines the execution order of the application pipeline.}{13}

Central to the ray marching rendering approach is the use of a \textbf{screen quad}. In this project, since both geometry and rendering processes are encapsulated within the fragment shader, only a basic quad covering the entire screen is necessary. This screen quad acts as the canvas on which all scene elements are rendered.


The application pipeline in OpenGL, following the flowchart in Figure \ref{pipeline_order}, is implemented in C++ and structured as follows:


\begin{enumerate}
    \item \textbf{Initialization}: The setup involves creating a window using GLFW, initializing OpenGL, and preparing a Vertex Array Object (VAO) and Vertex Buffer Object (VBO) for the screen quad. The screen quad vertices and indices are predefined constants.
    \item \textbf{Shader Compilation}: This step includes preprocessing includes and compiling shaders, complete with error reporting.
    \item \textbf{Render Loop}: The main loop involves rendering the screen quad, handling user inputs for interactive elements and camera movement, and sending parameters as uniforms to the GPU.
\end{enumerate}

% \todo{Make the diagram smaller and inline on the right}
% \JM{Make smaller, like fig 2.4}}

\subsection{Fragment Shader Execution Order}
\label{Fragment Shader Execution Order}

While procedural generation is a distinct stage in the conceptual workflow, in practice, it occurs \textbf{implicitly} within the implementation. The geometry is defined by implicit functions, which are in turn determined by various parameters. Once these parameters are set, the procedural generation phase is effectively complete. Therefore, the focus of the fragment shader’s execution order is on the rendering aspects of the scene.

\vspace{\baselineskip}

\minipagewrap{0.45}{shader_order}{}
{Outlines the execution order of the fragment shader.}{
Following the flowchart in Figure \ref{shader_order}, the fragment shader operates in this sequence:
\begin{enumerate}
    \item Compute the camera ray and the sun ray.
    \item Ray march and render terrain and trees.
    \item For pixels with an intersection with terrain or trees, determine the intersection with the water surface and render the water.
    \item For pixels without intersections, render the sun disk.
    \item Simulate atmospheric Rayleigh scattering for each pixel, resulting in atmospheric perspective effects for intersected pixels, and determining the sky color for pixels without intersections.
    \item Render clouds.
\end{enumerate}
}
% \myfigure{0.8}{shader_order}{}
% {Outlines the execution order of the fragment shader.}

% Before exploring each natural element, it is important to first outline the universal setup processes, including the calculation of both the camera and sun rays.

\vspace{\baselineskip}

Implementation details for calculating the camera ray and the sun ray are provided in Appendix \ref{Ray}.


\section{Terrain}
\label{Terrain}

This section details the the implementation for the procedural generation and rendering of terrain in the project.

\subsection{Procedural Generation}
\label{Terrain Procedural Generation}

\subsubsection{Implementing fBm}
\label{Implement fbm}

As discussed in Section \ref{FBM}, we can construct fBm using fractal noise. The function $\text{fBm}_k(\mathbf{x}, n)$ is evaluated as outlined in Algorithm \ref{algo:fbm}. The specifics of the value noise implementation are detailed in Section \todo{REF Appendix}.

\begin{algorithm}
\caption{Compute fBm Value at a Point}
\label{algo:fbm}
\begin{algorithmic}[1]
\Function{fBm}{$\mathbf{x}, n, \text{initial\_params}, \text{adjustment\_factors}$}
    \State $\text{fBm\_value} \gets 0$
    \State $\text{layer\_params} \gets \text{initial\_params}$
    \For{$i = 0$ \textbf{to} $n-1$}
        \State $\text{fBm\_value} \gets \text{fBm\_value} + \Call{ComputeLayerValue}{\mathbf{x}, \text{layer\_params}}$
        \State $\text{layer\_params} \gets \Call{UpdateLayerParams}{\text{layer\_params}, \text{adjustment\_factors}}$
    \EndFor
    \State \Return $\text{fBm\_value}$
\EndFunction
\Statex
\Function{UpdateLayerParams}{$\text{current\_params}, \text{adjustment\_factors}$}
    \State $f_i, a_i, \mathbf{R}_i, \mathbf{o}_i \gets \text{current\_params}$
    \State $\alpha, \beta, \mathbf{R}, \mathbf{o} \gets \text{adjustment\_factors}$
    \State \Return $(f_i \cdot \alpha, a_i \cdot \beta, \mathbf{R} \mathbf{R}_i, \mathbf{o} + \mathbf{o}_i)$
\EndFunction
\Statex
\Function{ComputeLayerValue}{$x, \text{layer\_params}$}
    \State $f_i, a_i, \mathbf{R}_i, \mathbf{o}_i \gets \text{layer\_params}$
    \State $\text{transformed\_x} \gets f_i \cdot \mathbf{R}_i \textbf{x} + \mathbf{o}_i$
    \State \Return $a_i \cdot \Call{ValueNoise}{\text{transformed\_x}}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Heightmap Generation}

I used the a 2D fBm, $text{fBm}_{2}$, to generate a terrain heightmap $H(x,z)$:

\begin{equation}
    H(x,z) = v\cdot\text{fBm}_{2}(\frac{(x,z)}{h})
\end{equation}

where $v$ and $h$ are scaling factors, $\text{fBm}_{2}$ is implemented by Algorithm \ref{algo:fbm}. Parameters for the fBm are made adjustable in the application's UI, as shown in Figure \ref{combined_3D}.

\paragraph{Limitation}
The generated terrain, despite being infinite, lacks \textbf{variety}, typically manifesting as uniformly mountainous or plateau-like landscapes, as shown in Figure \ref{local_heightmap}.

\myfigure{0.6}{local_heightmap}{h!}
{The image on the right shows a heightmap generated from fBm. The image on the left shows the associated terrain.}

\subsubsection{Dual Heightmap}
\label{Dual Heightmap}

To make the terrain more varied, I introduce a dual heightmap system:

\begin{enumerate}
    \item \textbf{Local Heightmap} ($H_{l}$): Utilizes fBm with higher frequencies to detail terrain on a smaller scale.
    \item \textbf{Global Heightmap} ($H_{g}$): Utilizes fBm with lower frequencies to define the overarching terrain shape and the extent of rockiness.
\end{enumerate}

The combined heightmap is formulated as follows:

\begin{equation}
H(x,z) = H_{g}(x,z) + \text{normalize}(H_{g}(x,z))\times H_{l}(x,z)
\end{equation}

This equation ensures:

\begin{itemize}
    \item The \textbf{addition} of $H_{g}(x,z)$ establishes the general terrain contour.
    \item The \textbf{multiplication} by $\text{normalize}(H_{g}(x,z))$ modulates the impact of $H_{l}(x,z)$, influencing the terrain's rockiness or ruggedness. Lower values of $H_{g}$ lead to gentler terrain, while higher values induce more pronounced elevation changes, akin to mountainous or rocky landscapes.
\end{itemize}

In summary, the dual heightmap approach effectively blends local details with global terrain contours to create diverse and realistic landscapes, as clearly depicted in the Figure \ref{combined_heightmap} and \ref{combined_3D} for comparison.

\myfigure{0.6}{combined_heightmap}{h!}
{Comparison between slices of the local and combined heightmaps along the x-axis. The local heightmap alone shows repetitive terrain shapes, while the combined heightmap offers diverse landscapes with both mountainous and flat regions. The global heightmap (blue curve) largely dictates the overall terrain features.}

\myfigure{1.0}{combined_3D}{h!}
{The first image shows a mountainous terrain from the local heightmap. Terrain in the second image combines local and global heightmaps, featuring both mountains and plateaus. The third image displays the application's UI with adjustable terrain generation parameters.}


\subsubsection{Domain Distortion}
\label{Domain Distortion}

To create terrain with natural variation, my project applies \textbf{domain distortion} to the heightmap. Domain distortion operates by modifying the input domain of a function before evaluation. For a given function $f(x)$, the domain is first distorted by a function $g(x)$, yielding $f(g(x))$ instead of the $f(x)$.

For the heightmap of the terrain, such distortion must be spatially varied to produce interesting results and continuous to avoid abrupt changes. This is achieved by incorporating a secondary 2D fBm, $\text{fBm}^\prime_2$, leading to the modified heightmap:

\begin{equation}
    H_l(x,z) = v\cdot\text{fBm}_{2}\left(\frac{x,z}{h}+\gamma\cdot\text{fBm}^\prime_2(x,z)\right)
\end{equation}

where $\gamma$ is the distortion magnitude, and $\text{fBm}^\prime_2$ utilizes distinct parameters. They are all adjustable via the application’s UI.

This approach is grounded in the principle that natural landscapes are sculpted by processes such as erosion, sedimentary deposition, and tectonic activities. Domain distortion through $\text{fBm}^\prime_2$ gives the generated terrain variations that mimic these geological phenomena. The use of fBm is particularly advantageous as it layers noises of varying frequencies and amplitudes; lower frequencies mimic large-scale geological shifts, while higher frequencies emulate smaller-scale details like erosion. This multi-scale noise application is demonstrated in Figure \ref{distortion_scale}.

Choosing suitable parameter values for domain distortion is key. Figure \ref{distortion_heightmap} displays how varying the distortion magnitude, $\gamma$, impacts the heightmap. While fine-tuning these parameters encourages the emergence of natural-looking terrains, pushing the parameters beyond typical ranges can yield intriguing, alien landscapes, as depicted in Figure \ref{alien}.

\myfigurerow{0.50}{distortion_scale}{h!}
{Lower frequencies mimic large-scale geological shifts, while higher frequencies emulate smaller-scale details like erosion.}
{0.45}{alien}
{Exotic terrain features generated by domain distortion}

\myfigure{1.0}{distortion_heightmap}{h!}
{Impact of varying distortion magnitude $\gamma$ on a heightmap}


\subsection{Ray Marching}
\label{Terrain Raymarching}

In the terrain ray marching process, I optimized the fixed-step ray marching algorithm (Section \ref{Raymarching Heightmaps}) by incorporating early termination, dynamic step sizing, and binary search.

\paragraph{Early termination}

By halting the ray marching process under certain conditions, computational load can be reduced. In my implementation, the process terminates if:

\begin{enumerate}
    \item The marched distance surpasses a predefined maximum distance, $d_{\text{max}}$.
    \item The number of steps exceeds the maximum allowed, denoted as $N$.
    \item A sample point's height exceeds the maximum terrain height (including trees), $y_{\text{max}}$, and the camera ray is ascending ($\mathbf{r_c}.y > 0$). This is based on the understanding that no intersection will occur if the ray is moving away from the terrain.
\end{enumerate}

\paragraph{Dynamic step size}

The step size in ray marching is dynamically adjusted based on two factors:

\begin{enumerate}
    \item The step size linearly increases with the \textbf{distance} already marched, aligning with the Level of Detail (LOD) concept. Farther objects can have less detail without significantly impacting the visual quality. 

   % \begin{equation}
   % s_{i} = a + b \cdot d_i
   % \end{equation}
   
    \item The step size also scales with the \textbf{height} above the terrain. This heuristic allows faster traversal of areas far from the terrain. 

\end{enumerate}

The step size for the $i$th step, $s_i$, is defined as:

\begin{equation}
    s_{i} = a + b \cdot d_{i} + c \cdot (\mathbf{P}_i.y - H(\mathbf{P}_i.xz))
\label{terrain ray march}
\end{equation}

where $a$,$b$ and $c$ are adjustable parameters, and $d_i$ is the distance marched to the $i$th point.

% \mywrapfigure{0.6}{bs}{r}
% {Comparing linear interpolation and binary search for terrain intersections. The right diagram shows a 2-step binary search, where $\mathbf{M}_1$ is below the terrain, leading to $\mathbf{M}_2$ being the midpoint between $\mathbf{P}_2$ and $\mathbf{M}_1$. }{}

\vspace{\baselineskip}

\minipagewrap{0.4}{bs}{}
{Comparing linear interpolation and binary search for terrain intersections. The right diagram shows a 2-step binary search, where $\mathbf{M}_1$ is below the terrain, leading to $\mathbf{M}_2$ being the midpoint between $\mathbf{P}_2$ and $\mathbf{M}_1$.}{
The final stage in the ray marching process involves refining the intersection point for enhanced precision. Traditionally (Section \ref{Raymarching Heightmaps}), this is done through linear interpolation between the last two points. In my approach, I have improved this process by employing a binary search method, allowing for a more precise intersection determination, as illustrated in Figure \ref{bs}.
}

% \begin{algorithm}
% \caption{Binary Search Refinement for Terrain Intersections}
% \label{label:bs}
% \begin{algorithmic}
% \Function{BinarySearchRefinement}{$\mathbf{B}$, $\mathbf{A}$}
%     \For{$i = 0$ to $K - 1$}
%         \State $\mathbf{M} \gets 0.5 \cdot (\mathbf{B} + \mathbf{A})$
%         \State $\Delta h \gets \mathbf{M}.y - H(\mathbf{M}.xz)$

%         \If{$\Delta h < 0$}
%             \State $\mathbf{B} \gets \mathbf{M}$
%         \Else
%             \State $\mathbf{A} \gets \mathbf{M}$
%         \EndIf

%         \If{$|\Delta h| < \varepsilon$}
%             \State \textbf{break}
%         \EndIf
%     \EndFor
% \EndFunction
% \end{algorithmic}
% \end{algorithm}


% \JM{The algorithm seems unnecessary here; it is quite simple and also dmescribed in the text.}
% \todo{remove algo}



\subsection{Procedural Coloring}
\label{Terrain Procedural Texturing}

In my project, the terrain is categorized into \textbf{four distinct elevation-based regions}: mud (below water level), sand, grass, and rock (illustrated in Figure \ref{4regions}).

\myfigure{0.7}{4regions}{h}
{Cross-sectional representation of terrain illustrating the four elevation-based regions. Striped textures are applied to steep areas.}

\paragraph{Smooth Boundaries}
The elevation thresholds separating these regions are denoted as $h_{1} < h_{2} < h_{3}$. Rather than using rigid boundaries, each threshold incorporates a variation $\pm \delta_i$ to facilitate smooth transitions. This is achieved by using the $\text{smoothstep}$ function to interpolate between regions.

\paragraph{Dynamic Boundaries}
To enhance realism and avoid monotonous separation at fixed elevations, a 2D fBm function offsets these boundaries. Consequently, at any point $\mathbf{P}$, the modified boundary is expressed as:

\begin{equation}
    h_{i} + \text{fBm}_{2}(\mathbf{P}.xz) \pm \delta_i
\end{equation}

This dynamic boundary adjustment is demonstrated in action in Figure \ref{region_transition}.



\paragraph{The Grass Region}
In the grass region, coverage is determined by the slope's steepness. A predefined threshold controls where grass appears: grass is rendered only on surfaces where the normal's y-component is below the threshold, indicating flatter regions. Interpolation is used to ensure smooth transition. 

I introduced color variations by evaluating another 2D fBm on the point's xz coordinates. By smoothly transitioning between two distinct grass colors through interpolation, this technique offers a more dynamic and visually appealing depiction of grassy areas, as shown in Figure \ref{grass}.

\myfigurerow{0.315}{region_transition}{h!}
{Demonstrates smooth and dynamic boundary adjustments between regions.}
{0.65}{grass}
{Aerial view of the color-varied grass region. \textit{Left}: terrain with natural grass color variation, \textit{Right}: emphasizes color variations for clearer visualization in high-contrast black and white.}

\paragraph{Striped Rock Texture}
In steep areas, I have procedurally generated a rock texture resembling striped patterns found on mountains, as illustrated in Figure \ref{stripe}. This effect is created by horizontally stretching value noise, producing stripe-like but non-uniform patterns. The degree of texture visibility is controlled by interpolating between the textured appearance and the rock's base color using the y-component of the normal. This ensures that stripes are prominent in steep regions, with a seamless transition to standard rock texture on flatter surfaces. Considering that the texture only appears on steep regions, I used the point's xy components as UV coordinates for texture sampling. 

\textbf{Importantly}, the texturing process does not involve creating and sampling from an image; rather, it queries a function directly.

\myfigure{0.8}{stripe}{h!}
{\textit{Left}: the application of a striped pattern texture across the terrain. \textit{Right}: the effect of interpolating this texture with the steepness of the terrain, resulting in the striped pattern showing only on steep slopes.}



\subsection{Illumination and Shadows}

\subsubsection{Shading}
\label{Terrain Shading}

In shading the terrain, I utilized the Phong reflection model \cite{phong_illumination_1975}, complemented by Fresnel Reflection (Section \ref{Fresnel}). The Phong model requires several inputs: the intersection point determined through ray marching (Section \ref{Terrain Raymarching}), the material color via procedural coloring (Section \ref{Terrain Procedural Texturing}), the incident light direction from the sun's spherical coordinates (Section \ref{Sun Ray}), and the terrain normal, analytically derived from the heightmap (Appendix \todo{REF}).

% I followed Equations \ref{ambient}, \ref{diffuse} and \ref{specular} to calculate the ambient, diffuse, and specular terms in the Phong model. The model's parameters are adjustable in the application's UI.

For added realism, the Phong shading model is augmented with Fresnel Reflection. Using Schlick's approximation (Equation \ref{schlick}) to compute the Fresnel term, $F(\theta)$, the specular term within the Phong model, $I_s$, is modified to be:

\begin{equation}
I_{s} = k_s \cdot F(\theta) \cdot (\mathbf{r} \cdot \mathbf{-cr})^n \cdot I
\end{equation}

In my implementation, I bypassed the direct calculation of the incidence angle $\theta$ for Schlick's approximation; instead, I computed $\cos(\theta)$ directly using the negative dot product of the normal and the camera ray, $\mathbf{n} \cdot \mathbf{r_c}$.

\subsubsection{Shadows}
\label{Terrain Shadows}

% \mywrapfigure{0.5}{shadow_terrain}{r}
% {Illustration of soft shadow calculations for terrains. For the point in penumbra, its shadow ray doesn't intersect with the terrain, with $d_{\text{min}}$ incurred at $\mathbf{P}_2$.}{}

\minipagewrap{0.5}{shadow_terrain}{}
{Soft shadow calculations for terrains. For the point in penumbra, its shadow ray doesn't intersect with the terrain, with $d_{\text{min}}$ incurred at $\mathbf{P}_2$.}{
To make terrain cast soft shadows (as demonstrated in Figure \ref{terrain_shadow1}), I implemented a method for calculating the penumbra factor, $P$, based on the approach in Section \ref{Shadows}. Instead of measuring the actual distance from a shadow sample point to the terrain, I used vertical distance as a more efficient approximation. This method is visualized in Figure \ref{shadow_terrain} and implemented as outlined in Algorithm \ref{algo:soft shadow terrain}.

\vspace{\baselineskip}

The shadow intensity, $S$, is subsequently integrated into the terrain's lighting model by multiplying with the final color $\mathbf{c}$ of the fragment.
}

\begin{algorithm}
\caption{Terrain Shadow Calculation}
\label{algo:soft shadow terrain}
\begin{algorithmic}
\Function{terrain\_shadow}{$\mathbf{X}, \mathbf{r_s}$}
    \State $d_{\text{min}} \gets \infty$
    \For{$i = 0$ to $s_{\text{max}}$}
        \State $\mathbf{P}_i \gets \mathbf{X} + i \cdot s \cdot \mathbf{r_s}$
        \State $d_i \gets \mathbf{P}_i.y - H(\mathbf{P}_i.xz)$
        \State $d_{\text{min}} \gets \min(d_{\text{min}}, \frac{d_i}{i \cdot s})$
    \EndFor
    \State \Return $\text{smoothstep}(0, 1, d_{\text{min}})$
\EndFunction
\end{algorithmic}
\end{algorithm}

% \myfigure{1.0}{terrain_soft}{h!}
% {Comparison of terrain shadows.}

\mysubfigurerowthree{0.31}{terrain_shadow1}{h!}{No Shadows}
{terrain_shadow2}{Hard Shadows}
{terrain_shadow3}{Soft Shadows}
{Comparison of terrain shadows.}

\section{Trees}

This section details the the implementation for the procedural generation and rendering of trees in the project.

\subsection{Procedural Generation}
\label{Tree Procedural Generation}

In my project, each tree is modeled using the SDF (Section \ref{SDF}) of an ellipsoid, with added 3D fBm to mimic the foliage, and populated using domain repetition. 

\paragraph{SDF of Ellipsoids}
Instead of a simply scaling a sphere's SDF, I used a more accurate representation for ellipsoids \cite{quilez_ellipsoid_nodate}:

\begin{equation}
\text{SDF}_\text{ellipsoid}(\mathbf{X}, \mathbf{r}) = \frac{\left\|\frac{\mathbf{X}}{\mathbf{r}}\right\| \cdot \left(\left\|\frac{\mathbf{X}}{\mathbf{r}}\right\| - 1\right)}{\left\|\frac{\mathbf{X}}{\mathbf{r} \cdot \mathbf{r}}\right\|}
\end{equation}

Here, $\mathbf{r}$ is a vector specifying the ellipsoid's dimensions, and the origin is assumed to be the center of the ellipsoid.

\paragraph{fBm for Leaves}
The leaves are simulated by distorting the ellipsoid with a 3D fBm:

\begin{equation}
\text{SDF}_\text{tree}(\mathbf{X}) = \text{SDF}_\text{ellipsoid}(\mathbf{X}, \mathbf{r}) + \gamma \cdot \text{fBm}_3(\mathbf{X})
\end{equation}

Here, $\gamma$ represents the distortion magnitude, and it is adjustable in the application's UI along with the fBm parameters. The dimension $\mathbf{r}$ of the ellipsoids is randomized within a range to introduce size variation among trees.

\paragraph{Domain repetition for tree population}
To efficiently distribute trees across the terrain, a domain repetition \cite{quilez_domain_nodate} technique is used. The SDF for a tree instance is made periodic on an infinite axis-aligned 2D square grid in the xz-plane. This results in a representation of infinite trees within this grid pattern, as illustrated in Figure \ref{domain_rep}. The tree positions are vertically adjusted by the terrain height and an additional constant $v$:

\begin{equation}
\begin{aligned}
\text{SDF}_\text{trees}(\mathbf{X}) &= \text{SDF}_\text{tree}(\mathbf{X} - \mathbf{C}), \\
\text{where } \mathbf{C}.x &= \mathbf{C}.z = \left(\left\lfloor \frac{\mathbf{x}}{d} \right\rfloor + 0.5\right) \cdot d, \\
\mathbf{C}.y &= H(\mathbf{X}.{xz}) + v.
\end{aligned}
\end{equation}

Here, $\mathbf{C}$ represents the center of the cell that $\mathbf{X}$ is in, and $d$ represents the cell size in the square grid. 

Moreover, trees are placed only on terrain that meets two criteria: the slope must be below a certain threshold to avoid cliffs, and the elevation must be higher than the water surface to prevent trees in water.

\paragraph{Randomization and Species Variation}
To avoid a uniform layout, a random offset $o(\mathbf{X})$, based on the position hashed, is introduced:

\begin{equation}
\begin{aligned}
\text{SDF}_\text{trees}(\mathbf{X}) &= \text{SDF}_\text{tree}(\mathbf{X} - \mathbf{C}), \\
\text{where } \mathbf{C}.x &= \mathbf{C}.z = \left(\left\lfloor \frac{\mathbf{X}}{d} \right\rfloor + 0.5\right) \cdot d + o(\mathbf{X}), \\
\mathbf{C}.y &= H(\mathbf{X}.{xz}) + v.
\end{aligned}
\end{equation}

Moreover, two species of trees (A and B) are introduced to add variety. The species type is determined using a 2D fBm; if $\text{fBm}(\mathbf{X})$ is below a certain threshold, the tree is of species A, otherwise species B. Each species has a different range for the randomized dimensions $\mathbf{r}$, allowing for distinct size characteristics between the species. The randomization of offsets and dimensions are depicted in Figure \ref{domain_rep}.

\paragraph{Considering Neighboring Cells}
The random offset $o(\mathbf{X})$ introduces complexity in determining the closest tree to any given point $\mathbf{X}$, as it may not necessarily be within the same cell as $\mathbf{X}$. To address this, I evaluate the surrounding eight cells, along with the cell containing $\mathbf{X}$, to find the nearest tree. The SDF for the trees, therefore, is calculated by finding the minimum distance to a tree among these nine cells:

\begin{equation}
\begin{aligned}
\text{SDF}_\text{trees}(\mathbf{X}) &= \min_{i\in\{-1,0,1\}}\min_{j\in\{-1,0,1\}}\text{SDF}_\text{tree}(\mathbf{X} - \mathbf{C_{ij}}), \\
\text{where } \mathbf{C_{ij}}.x & = \left(\left\lfloor \frac{\mathbf{X}}{d} \right\rfloor + i + 0.5\right) \cdot d + o(\mathbf{X}), \\
\mathbf{C_{ij}}.z & = \left(\left\lfloor \frac{\mathbf{X}}{d} \right\rfloor + j + 0.5\right) \cdot d + o(\mathbf{X}), \\
\mathbf{C_{ij}}.y &= H(\mathbf{X}.{xz}) + v.
\end{aligned}
\end{equation}

\myfigure{1.0}{domain_rep}{h!}
{Depicts domain repetition and the incorporation of randomized offsets and dimensions in tree generation, excluding species variation for simplicity.}

\subsection{Ray Marching}
\label{Tree Ray Marching}

While Sphere Tracing (Section \ref{Raymarching SDFs}) is a viable method for determining the intersection with the SDF of trees, initiating this process from the camera position would be inefficient, considering that a similar ray marching process has just been done for the terrain.

To optimize efficiency, I integrated checks for intersections with tree canopies into the terrain ray marching algorithm. The tree canopy, defined by a constant height above the terrain, serves as an approximated bounding box for the trees, as depicted in Figure \ref{canopy}. Once the terrain intersection is identified through ray marching, the farthest canopy intersection point is then used as the starting position for Sphere Tracing of the trees. This approach substantially reduces sphere tracing steps, as illustrated in Figure \ref{canopy}. 

\myfigure{1.0}{canopy}{h!}
{Illustrating the efficiency of my tree ray marching method: Sphere Tracing from the camera (left) takes 3 steps, while starting from the canopy intersection (right) requires only 1 step.}

\subsection{Procedural Coloring}

In the procedural coloring of trees, the primary objective is to introduce \textbf{color variation}cc among trees. The resulting variations are shown in Figure \ref{tree}.

I implemented two types of color variations: inter-species and intra-species. Each generated tree belongs to either species A or B (Section \ref{Tree Procedural Generation}), leading to the first type of variation, where different color schemes distinguish between the two species. Within each species, I simulated color variation due to age differences by generating a random value representing a tree's age. This value is used to interpolate between the 'young' and 'old' colors specific to the species.

\myfigure{0.6}{tree}{h!}
{\textit{Left}: trees of one species in more saturated colors and another species in less saturated colors, with color variations indicating age differences. \textit{Rigth}: uses higher contrast colors to differentiate the two species.}

\subsection{Illumination and Shadows}

\subsubsection{Shading}

For effective lighting of trees, determining the normal at the intersection point $\mathbf{P}$ is crucial. First, I obtain the normal of the tree's SDF using a numerical method (Appendix \todo{REF}). Subsequently, this SDF normal is blended with the normal of the underlying terrain at point $\mathbf{P}$. This blending facilitates the representation of tree lighting that conforms to the shape of the terrain beneath, expressed as:

\begin{equation}
    \mathbf{n}(\mathbf{P}) = a \cdot \mathbf{n}_{\text{SDF}}(\mathbf{P}) + (1 - a) \cdot \mathbf{n}_H(\mathbf{P}.xz)
\end{equation}

Here, $\mathbf{n}_{\text{SDF}}(\mathbf{P})$ is the SDF normal at point $\mathbf{P}$, $\mathbf{n}_H$ is the normal of the underlying terrain, and $a$ is a blending factor.

Similar to the shading approach for terrain (Section \ref{Terrain Shading}), the Phong Model with Fresnel reflection is employed for shading trees. Additionally, to approximate self-occlusion and ambient occlusion by nearby trees, I introduce an occlusion term $O$, computed as:

\begin{equation}
O = \text{smoothstep}(a, b, h_t)
\end{equation}

Here, $O$ is interpolated between 0 and 1 based on the height of the tree $h_t$, with parameters $a$ and $b$ controlling the interpolation. The diffuse component of the lighting model is then multiplied with $O$, resulting in lower parts of the tree appearing less lit than the upper parts, as demonstrated in Figure \ref{tree_illumination}.

\subsubsection{Shadows}
\label{Tree Shadows}

To make trees cast soft shadows, I adapted the technique used for terrains (see Section \ref{Terrain Shadows}), with adjustments to account for the translucent nature of tree foliage. Unlike solid terrain, tree foliage does not completely block light. I approximate the extent of shadowing with the degree of penetration of the shadow ray into the tree's volume. This is based on the principle that deeper penetration into the tree, likely indicating proximity to the trunk, results in deeper shadows.

To achieve this effect, following Algorithm \ref{algo: tree shadow}, I accumulate the value $(\text{SDF}_{\text{trees}}(\mathbf{P_s}) - \lambda) \cdot t$ along the shadow ray path. This accumulation, represented by $D$, is used to determine the shadow intensity. Here, $\mathbf{P_s}$ represents points along the shadow ray, $t$ is the distance of the ray, and $\lambda$ serves as a threshold for considering tree SDF in shadow calculations. The resultant accumulated value, $D$, is then interpolated using a smoothstep function to calculate the shadow intensity, $S$. If $D \le D_{\text{min}}$, the point is classified as being in umbra, fully shadowed. The result is demonstrated in Figure \ref{tree_illumination}.

\begin{algorithm}
\caption{Tree Shadow Calculation}
\label{algo: tree shadow}
\begin{algorithmic}
\Function{TreeShadow}{$\mathbf{P}$}
    \State $D \gets 0$
    \For{$t = 0$ to $t_{\text{max}}$ step $s$}
        \State $\mathbf{P}_s \gets \mathbf{P} + t \cdot \mathbf{r_s}$
        \State $d \gets \text{tree\_sdf}(\mathbf{P}_s)$
        \If{$d < \lambda$}
            \State $D \gets D + (d - \lambda) \cdot t$
        \EndIf
        \If{$D \leq D_{\text{min}}$}
            \State \textbf{break}
        \EndIf
    \EndFor
    \State $S \gets \text{smoothstep}(D_{\text{min}}, 0, D)$
    \State \Return $S$
\EndFunction
\end{algorithmic}
\end{algorithm}

\myfigure{1.0}{tree_illumination}{h!}
{Illustrating the complete process of tree illumination and shadowing.}

\section{Water}

This section details the the implementation for the representation and rendering of water in the project.


\subsection{Representation}

This project used a simple model to represent water. Assuming a constant water level globally, the water surface is modeled as a plane perpendicular to the y-axis. The elevation of this surface, denoted as $w$, can be adjusted in the UI. Instead of geometrically distorting the water surface to simulate waves, normal mapping \cite{blinn_simulation_1978} was applied to mimic wave effects, as detailed in Section \ref{Water Shading}.

\subsection{Finding Intersection}

In the shader execution order (Section \ref{Fragment Shader Execution Order}), the intersection with the water is determined \textbf{after} the identification of terrain and tree intersections, as illustrated in the flowchart in Figure \ref{shader_order}. This order is chosen because water is transparent, and its visibility depends on whether it is occluded by these elements.

The visibility test checks if the camera ray intersects with the water surface before any terrain or tree. If the intersection point with either a tree or terrain is beneath the water level $w$, the water is considered visible.

To calculate the intersection with the water surface, an analytical approach is used. The intersection point $\mathbf{P}_w$ and distance $d_w$ from the camera to the water surface are calculated by:

\begin{equation}
d_{w} = \frac{w - \mathbf{C}.y}{\mathbf{r_c}.y}
\end{equation}

and

\begin{equation}
\mathbf{P}_{w} = \mathbf{C} + d_{w}\cdot\mathbf{r_c}
\end{equation}

where $\mathbf{C}$ denotes the camera position, $\mathbf{r_c}$ the normalized direction of the camera ray, and $w$ the water level.

\subsection{Procedural Coloring}
\label{Water Procedural Coloring}

The procedural coloring of water focuses on two visual aspects: depth-dependent color variation and transparency. 

\paragraph{Depth-Based Color Variation}
   
The effective length of water that the camera ray travels through, denoted as $l$, is computed as the difference between the total distance to an object $d_{o}$ and the distance to the water surface $d_{w}$:

\begin{equation}
   l = d_{o}-d_w
\end{equation}

The \textbf{transmittance} of water, $\tau$, quantifies the amount of light that passes through the water. It decreases exponentially with the increase in $l$, modulated by a decay factor $k$:

\begin{equation}
    \tau = \exp(-k \times l)
\end{equation}
   
Water color is then interpolated between deep and shallow hues based on this transmittance $\tau$, emulating how water absorbs and scatters light differently at varying depths.

\paragraph{Transparency Effect}

Transparency is achieved by blending the water material color with the color at the underwater terrain intersection, based on the transmittance $\tau$. A higher $\tau$ indicates more light scatter, resulting in less light penetration and decreased transparency, and vice versa.

\subsection{Illumination}
\label{Water Shading}

A heightmap $H(x,z)$, constructed from a 2D fBm, represents water surface waves. The normal of this heightmap, $N_H$, serves as the normal map. The normal vector $\mathbf{n}$ at a point $\mathbf{X}$ on the water surface is:

\begin{equation}
\mathbf{n}= N_{H}(\mathbf{X}.xz)
\end{equation}

Details on calculating heightmap normals are in Appendix \todo{REF}.

Shading follows similar techniques as for terrain, utilizing the Phong model with a Fresnel effect, as detailed in Section \ref{Terrain Shading}. 

% Reflections, although not implemented, could be approximated by rendering from the intersection point, using the reflected ray as the camera ray, limiting recursive renders, and weighting the reflection results progressively less. \question{Shall I talk about how reflections could be done in Conclusion: Future Extensions?}

\subsection{Example Output}

\mysubfigurerowthree{0.31}{water1}{h!}
{}
{water2}{}
{water3}{}
{Example renders of water scenes in my application.}

\section{Atmosphere}
\label{Atmosphere}

This section outlines the physics-based rendering technique implemented for simulating atmospheric effects, primarily focusing on Rayleigh scattering \cite{young_rayleigh_1981}. This phenomenon, caused by the interaction of sunlight with minuscule atmospheric particles, is crucial for achieving authentic sky colors and atmospheric perspective, as demonstrated in renders in Figure \ref{atmosphere}  and \ref{atmospheric} respectively.

The simulation examines the path of light from a point $\mathbf{E}$ in the scene to the camera at position $\mathbf{C}$, counter to the direction of the camera ray $\mathbf{r_c}$. It accounts for two key phenomena, as illustrated in Figure \ref{scatter}:
\begin{itemize}
    \item \textbf{In-scattering}: Light scatters from other directions into the direction of the ray.
    \item \textbf{Out-scattering}: The loss of light from the ray as it scatters in other directions.
\end{itemize}

\myfigure{0.5}{scatter}{h!}
{Illustrates the in-scattering and out-scattering effects as the light travels through the atmosphere from point $\mathbf{E}$ to the camera at $\mathbf{C}$.}


\subsection{Modeling the Atmosphere}

Since the procedurally generated world is infinite in the xz direction, the atmospheric density depends solely on altitude. The atmosphere is given an upper limit $h_{\text{max}}$, above which the density is zero. The atmospheric density $D$ at a point $\mathbf{P}$ below the upper bound is modeled as:

\begin{equation}
    D(\mathbf{P})=\frac{e^{-\lambda \mathbf{P}.y}\left(h_{\text{max}}-\mathbf{P}.y\right)}{h_{\text{max}}}
\end{equation}

where $\lambda$ is a decay constant modifiable in the UI.

\subsection{The In-Scattering Equation}
\label{The In-Scattering Equation}

The in-scattering equation \cite{nishita_display_1993} models the accumulation of sunlight along a ray from point $\mathbf{E}$ to the camera $\mathbf{C}$ through the atmosphere. It encompasses both in-scattering and out-scattering. Figure \ref{inscatter} provides a visual aid for understanding the related notations.

The equation is expressed as:

\begin{equation}
    I_v(\lambda) = I(\lambda) \times K(\lambda) \times F(\theta, g) \times \int_{\mathbf{C}}^{\mathbf{E}} \left( D(\mathbf{P}) \times \exp\left(-t\left({{\mathbf{PS}},\lambda}\right) -t \left({{\mathbf{PC}},\lambda}\right) \right)\right) ds
\end{equation}

where:
\begin{itemize}
    \item $I(\lambda)$ denotes the intensity of sunlight at wavelength $\lambda$.
    \item $K(\lambda)$ is the wavelength-dependent scattering coefficient. In the implementation, the coefficients for the red, green, and blue wavelengths are used, with their values being adjustable in the UI.
    \item $F(\theta, g)$ is the phase function, representing the angular distribution of scattering.
\end{itemize}

The end point $\mathbf{E}$ varies based on scene interaction:
\begin{itemize}
    \item If the camera ray intersects with an object (terrain, tree, or water), $\mathbf{E}$ is the intersection point, as shown in the right diagram of Figure \ref{inscatter}.
    \item If there's no intersection, $\mathbf{E}$ is the point where the camera ray $\mathbf{r_c}$ intersects the upper boundary of the atmosphere, as shown in the left diagram of Figure \ref{inscatter}.
\end{itemize}

In the integral:
\begin{itemize}
    \item $\mathbf{P}$ is a point on the segment between $\mathbf{C}$ and $\mathbf{E}$. $P = \mathbf{R}(s) = \mathbf{C} + s \cdot \mathbf{r_c}$, where $\mathbf{R}$ is the camera ray.
    \item $t$ is the out-scattering function.
    \item $\mathbf{S}$ is the intersection between the atmosphere's upper boundary with the ray with origin $\mathbf{P}$ and direction $\mathbf{r_s}$, as depicted in Figure \ref{inscatter}.
    \item $H_0$ is a parameter adjustable in the UI.
\end{itemize}

\myfigure{1.0}{inscatter}{h!}
{Visually clarify the notations and concepts related to the in-scattering equation. \textit{Left}: a situation where the camera ray does not intersect any scene elements, \textit{Right}: a situation with an intersection.}

\vspace{0.5em}
The integral is approximated using Riemann sums \cite{hugheshallett_applied_2009}, a method of summing rectangular areas to approximate the area under a curve. This technique involves marching along the camera ray, sampling at points $\mathbf{P}_i$ and evaluating the integrand at each point, then multiplying by the step size $s$. The sum of these calculations approximates the integral in the scattering equation. This process is illustrated in Figure \ref{riemann}.

\myfigure{1.0}{riemann}{h!}
{Visually demonstrates the Riemann sums method for the in-scattering equation. \textit{Left}: a situation where the camera ray does not intersect any scene elements, \textit{Right}: a situation with an intersection.}

\subsubsection{The Out-Scattering Function}

The out-scattering function $t$ describes the reduction of light as it traverses through the atmosphere and is given by:

\begin{equation}
    t(\mathbf{A}, \mathbf{B}, \lambda) = 4\pi \times K(\lambda) \times \int_{\mathbf{A}}^{\mathbf{B}} D(\mathbf{P}) ds
\end{equation}

\paragraph{Optical Depth}
\label{Optical Depth}

The integral represents the optical depth, essentially the atmospheric density averaged along the path segment multiplied by the length of the segment. This value indicates the cumulative effect of atmospheric particles on light along the ray path: more particles imply greater scattering of light away from the ray.

To approximate the optical depth, the Riemann sum method was employed, similar to the approach outlined in Section \ref{The In-Scattering Equation}.

\paragraph{The Phase Function}

The phase function $F$, essential in determining the directionality of scattering, is adapted from the Henyey-Greenstein function \cite{nishita_display_1993}:

\begin{equation}
    F(\theta, g) = \frac{3 \times (1 - g^2)}{2 \times (2 + g^2)} \times \frac{1 + \cos^2 \theta}{(1 + g^2 - 2 \times g \times \cos \theta)^{\frac{3}{2}}}
\end{equation}

Here, $\theta$ is the angle between the incoming light and the scattering direction. The asymmetry parameter $g$ influences the scattering distribution. For Rayleigh scattering, where $g = 0$, the function simplifies to $\frac{3}{4}(1 + \cos^2\theta)$, indicating more light scattered in directions close to the incoming light source.

\subsection{Blending}

The blending process involves combining the in-scattered light with light emitted at intersections in the scene. When the camera ray doesn't intersect any scene object, the sky color is determined solely by in-scattered light. However, at intersection points, the blending of in-scattered and emitted light is necessary, given by \cite{nishita_display_1993}:

\begin{equation}
    I'_v(\lambda) = I_v(\lambda) + I_e(\lambda)\exp(-t(\mathbf{C}, \mathbf{E}, \lambda))
\end{equation}

where $I_v(\lambda)$ represents the in-scattered light at wavelength $\lambda$, and $I_e(\lambda)$ is the light emitted at the intersection $\mathbf{E}$. 

For the final blended color $\mathbf{c}$ with RGB representation, I focus on three specific wavelengths corresponding to red, green, and blue light:

\begin{equation}
\mathbf{c} = (I_{v}'(\lambda_r), I_{v}'(\lambda_g), I_{v}'(\lambda_b))
\end{equation}

\myfigure{1.0}{atmosphere}{}
{Displays a series of screenshots with varying sun directions and altitudes: the first row captures lower altitudes, while the second showcases higher altitudes. Progressing from left to right, the sun's angle shifts from being more perpendicular to more oblique relative to the ground, mirroring the natural shift from midday to evening.}

\myfigure{0.7}{atmospheric}{}
{Compares renders before and after applying the atmosphere simulation, emphasizing the atmospheric perspective on the terrain. Note how the distant terrain appears more merged with the sky due to this effect.}

\section{Clouds}

This section details the the implementation for the procedural generation and rendering of clouds in the project.

\subsection{Procedural Generation of Clouds}

In modeling clouds, a \textbf{volumetric} approach is essential due to their complex, three-dimensional structure. Unlike terrain, clouds are not just surfaces; they are composed of particles with varying densities in three-dimensional space. Therefore I used a density map, $D(\mathbf{X})$, to represent the density at any given point $\mathbf{X}$ in 3D space.

The procedural generation of this map begins with establishing a \textbf{bounding box} for the clouds, extending infinitely in the xz-plane and bounded vertically between lower $y_l$ and upper $y_u$ limits.

The generation involves two primary steps (as illustrated in Figure \ref{cloud_fbm}):
\begin{enumerate}
    \item The \textbf{base density} of the clouds is initially determined relative to their vertical position. The cloud density is intensified nearer to the box's central vertical axis, denoted by $y_c$, the mean of $y_u$ and $y_l$. The base density at a point $\mathbf{x}$ is formulated as:

\begin{equation}
    D(\mathbf{X}) = h - |\mathbf{X}.y - y_{c}|
\end{equation}

    where $h$ is a fraction of the total vertical extent of the cloud box.

    \item To this base density, a \textbf{3D fBm} is added, introducing natural, cloud-like irregularities and variations. The final density $D(\mathbf{x})$ at point $\mathbf{x}$ is then:

\begin{equation}
    D(\mathbf{X}) = h - |\mathbf{X}.y - y_{c}| + \text{fBm}_3(\mathbf{X})
\end{equation}
\end{enumerate}

In this approach, the combination of a base density gradient and the nature-simulating properties of fBm (Section \ref{FBM}) results in a realistic representation of cloud density. This method ensures clouds exhibit natural variations, while predominantly clustering towards the bounding box's center, avoiding unrealistic overflow.

\myfigure{0.8}{cloud_fbm}{h!}
{Illustrates the procedural generation process of the cloud density map. The diagram displays cross-sectional slices of the density map along the x-axis.}

\subsection{Volumetric Rendering}

Due to their transparent and volumetric properties, clouds cannot be rendered just using traditional ray marching to locate surface intersections. Instead, my approach involves sampling multiple points along the camera ray within the cloud volume. At each sampled point, color is calculated, and these colors are subsequently blended to form the final color. It’s important to note that this rendering technique is based on heuristic methods rather than a physically accurate simulation of light scattering phenomena, such as Mie Scattering.

\subsubsection{Volumetric Ray Marching}

The volumetric ray marching process involves calculating the start and end points of the ray within the cloud's bounding box and then marching the ray through the volume.

\paragraph{Start and end points}

The start $t_s$ and end $t_e$ distances along the ray from the camera position $\mathbf{C}$ are determined by intersecting the ray with the cloud's vertical boundaries, $y_l$ and $y_u$: 

\begin{equation}
    t_s = \max(\min(\frac{y_l - \mathbf{C}.y}{\mathbf{r_c}.y}, \frac{y_u - \mathbf{C}.y}{\mathbf{r_c}.y}), 0)
\end{equation}

\begin{equation}
    t_e = \min(\max(\frac{y_l - \mathbf{C}.y}{\mathbf{r_c}.y}, \frac{y_u - \mathbf{C}.y}{\mathbf{r_c}.y}), t_{\text{max}})
\end{equation}

Here $\mathbf{r_c}$ is the direction of camera ray, and $t_{\text{max}}$ is the maximum ray marching distance. The computations consider all the cases: where the camera position is below, in or above the bounding box.

\paragraph{Adaptive step size}

The step size $s$ is dynamically adjusted based on the cloud density $D(\mathbf{X})$ and the distance $t$ already covered by the ray. The density function $D(\mathbf{X})$ includes negative values, which, while uncharacteristic for physical densities, are utilized here for computational efficiency. These negative values indicate regions devoid of clouds, allowing the algorithm to increase the step size $s$ in proportion to the negativity of the density, thereby speeding up the march through empty spaces.

\begin{equation}
    s = a \times |D(\mathbf{X})| + s_{\text{min}}
\end{equation}

 Here, $a$ is a scaling factor for density, and $s_{\text{min}}$ is the minimum step size.

For positive density regions, the step size is also increased with distance $t$ to reduce computational load, as distant samples contribute less to visual detail and the final color blend.

\begin{equation}
     s = \max(s_{\text{min}}, b \times t)
\end{equation}

where $b$ is the distance scaling factor.

\paragraph{Termination criteria}

Ray marching is terminated under one of three conditions: 

\begin{enumerate}
    \item The ray exits the bounding box ($t\ge t_e$).
    \item A maximum number of steps is reached.
    \item The accumulated alpha value of the blended color reaches a threshold indicating sufficient opacity (as detailed in Section \ref{Cloud Blending}).
\end{enumerate}

\subsubsection{Illumination and Shadows}

Lighting is calculated at every sample point in the volumetric rendering process. The Phong model \cite{phong_illumination_1975} is adapted here by excluding the specular term, since the primary visual characteristics of particles in clouds are diffuse scattering and translucency. 

Shadows within the clouds, resulting from cloud particles obstructing light from other particles, are rendered using a technique called volumetric shadowing. This process involves marching a ray along the direction of the sun ray and accumulating the density of cloud particles at various points along this path. Higher densities indicate a greater number of particles blocking the light, resulting in deeper shadows at the sample point. Conversely, lower densities suggest fewer particles and less shadowing

\subsubsection{Blending}
\label{Cloud Blending}

The blending is done cumulatively, starting from the nearest point to the viewer and progressing towards the back. A cumulative color vector $\mathbf{c}$ (with RGBA components) is maintained and updated along the ray path.

For each sampled point $\mathbf{X_i}$, the alpha value $\alpha$ is calculated based on the density $D(\mathbf{X}_i)$ at that point, the step size $s$, and a scaling factor $\lambda$, with an upper limit set by the maximum cumulative alpha $\alpha_{\text{max}}$:

\begin{equation}
  \alpha = \text{clamp}(\lambda \times s \times D(\mathbf{X}_i), 0, \alpha_{\text{max}})
\end{equation}

The cumulative color $\mathbf{c}$ is then updated by blending with the color $\mathbf{c}_i$ at each sampled point $\mathbf{X_i}$:
  
\begin{equation}
  \mathbf{c} \mathrel{+}= \mathbf{c}_i \times \alpha \times (\alpha_{\text{max}} - \mathbf{c}.\alpha)
\end{equation}

Here, $\alpha$ prioritizes points with higher density, while the factor $(\alpha_{max} - \mathbf{c}.\alpha)$ prioritizes closer points. Finally, the cloud color is blended with the fragment color using $\mathbf{c}.\alpha$.


\subsubsection{Sunlight Penetration Effect}

An additional effect emulates sunlight filtering through clouds, especially noticeable when the sun is behind them, as demonstrated in Figure \ref{clouds_sun}. This is achieved by augmenting $\mathbf{c}$ with the sun's color $\mathbf{c}_{\text{sun}}$, modulated by $\mathbf{c}.\alpha$, and the angle between the camera ray direction $\mathbf{r_c}$ and the sun direction $\mathbf{r_s}$:

\begin{equation}
  \mathbf{c}.rgb \mathrel{+}= a \cdot \mathbf{c}_{\text{sun}} \cdot (1 - b \cdot \mathbf{c}.\alpha) \cdot (\text{clamp}(\mathbf{r_c} \cdot \mathbf{r_s}, 0, 1))^ c
\end{equation}

Here, $a,b,c$ are parameters that are adjustable in the UI. $\mathbf{c}.\alpha$ indicates the cumulative cloud density along the camera ray, with a lower value indicating less cloud density and therefore more sunlight filtering through. Moreover, the effect is stronger when the camera ray is closely aligned with the sun ray, indicated by $\mathbf{r_c}\cdot \mathbf{r_s}$.

% \myfigure{0.8}{clouds_sun}{h!}
% {Comparison of cloud renders demonstrating sunlight penetration effect.}

\mysubfigurerow{0.4}{sunlight0}{h!}
{Effect off.}
{0.4}{sunlight1}
{Effect on.}
{Comparison of cloud renders demonstrating sunlight penetration effect.}
{clouds_sun}

\section{Procedural Planets}

Since development was on schedule, I had time to implement an extension: applying the existing procedural generation and rendering of natural elements to planets. This section explains how I generate planet heightmaps, render the planet surface, and incorporate water and atmosphere.


\vspace{\baselineskip}
\minipagewrap{0.62}{planet_heightmap}{}
{Planet heightmap.}{
\subsection{Heightmap Generation}
\vspace{\baselineskip}
In my project, the heightmaps of planets differ from those of flat terrains. For a planet centered at point $\mathbf{O}$, given a direction $\mathbf{OP}$, the point $\mathbf{I}$ is where the ray in the direction of $\mathbf{OP}$ intersects the planet's surface. The heightmap function $H_{\text{planet}} (\mathbf{P})$ returns the length $\|\mathbf{OI}\|$, as illustrated in Figure \ref{planet_heightmap}.
}

To calculate $H_\text{planet} (\mathbf{P})$, I used triplanar projection \cite{flick_triplanar_nodate} to project 3 heightmaps onto a sphere, computed as:

\begin{equation}
\begin{aligned}
H_\text{planet} (\mathbf{P}) = 
&H(\mathbf{P}.zy + f(\operatorname{sgn}(\mathbf{P}.x))) \times w_{x}+ \\
&H(\mathbf{P}.xz + f(\operatorname{sgn}(\mathbf{P}.y))) \times w_{y}+ \\
&H(\mathbf{P}.xy + f(\operatorname{sgn}(\mathbf{P}.z))) \times w_{z}
\end{aligned}
\end{equation}

Here, $H$ is the terrain heightmap (Section \ref{Terrain Procedural Generation}), the function $f$ ensures that opposite sides of the planet do not share the same elevation, and the weights $w_x,w_y,w_z$ are computed as follows:

\begin{equation}
\begin{aligned}
w_{x} &= \hat{|\mathbf{OP}}.x|^{k} \\
w_{y} &= \hat{|\mathbf{OP}}.y|^{k} \\
w_{z} &= \hat{|\mathbf{OP}}.z|^{k} \\
\label{tri weights}
\end{aligned}
\end{equation}

These weights are then normalized to sum up to 1. The parameter $k$ controls the blending sharpness between heightmaps, as illustrated in Figure \ref{k1}.


\mysubfigurerowthree{0.31}{k1}{h!}
{$k=1$}
{k10}{$k=10$}
{k100}{$k=100$}
{Comparison of triplanar-projected heightmaps with varying sharpness (\( k \)). Surface colors represent heightmap values, where black indicates high values and white indicates low values.}

\subsection{Ray Marching}

Similar to terrain ray marching (Section \ref{Terrain Raymarching}), I used fixed-step ray marching for the planet's heightmap, incorporating early termination and dynamic step sizes. As an additional optimization, if the camera is outside the planet's bounding sphere, I begin ray marching from the camera ray's intersection with this sphere.

\subsection{Procedural Coloring, Illumination and Shadows}

The procedural coloring, illumination, and shadow techniques are largely similar to those used for flat terrain (Section \ref{Terrain}), with a few notable differences.

Sinec the planet heightmap is a combination of three separate heightmaps, their normals need to be combined as well, a technique called triplanar normal mapping. First, the Whiteout Blend method (CITE) is used to blend the three normals along with the normal of the sphere at point $\mathbf{P}$, denoted as $\hat{\mathbf{OP}}$. The three normals are transformed from tangent space to world space and then blended using the weights for heightmap triplanar projection (Equation \ref{tri weights}). The code for this normal blending process is provided in Appendix \todo{REF}.

This approach ensures smooth and accurate normal calculations, as illustrated in Figure \ref{normal1}.

\mysubfigurerowthree{0.31}{normal1}{h!}
{Sphere normal.}
{normal2}{Planet normal.}
{normal3}{Shaded planet.}
{Visualizations of triplanar normal mapping.}

\subsection{Other Natural Elements}

\subsubsection{Water}

For planets, I model water surfaces as spheres rather than planes. To render water, I first calculate the intersection between the camera ray and the water surface (Appendix REF details how to find the intersection between a ray and a sphere). Procedural coloring (Section \ref{Water Procedural Coloring}) and illumination (Section \ref{Water Shading}) are then applied similarly to how they are for water on flat terrain.

\subsubsection{Spherical Atmosphere}

The atmospheric density is still modeled as an exponential function based on altitude. However, the atmosphere is bounded by a sphere instead of a plane. For approximating the in-scattering equation, I only consider the segment of the ray that lies within the bounding sphere.

\subsection{Example Output}

Figure \ref{planet1} and \ref{planet4} display various planetary scenes rendered by my application. Refer to the accompanying video submission to see real-time navigation through these scenes.

\mysubfigurerowthree{0.31}{planet1}{h!}{}
{planet2}{}
{planet3}{}
{Example renders of an earth-like procedural planet in my application.}

\vspace{\baselineskip}
\vspace{\baselineskip}

\mysubfigurerowthree{0.31}{planet4}{h!}{}
{planet5}{}
{planet6}{}
{Example renders of various procedural planets in my application.}

\section{Input Controls}

In this project, I implemented both a first-person, game-like camera control model \cite{noauthor_first-person_2024} and trackball camera controls \cite{noauthor_object_nodate}. Additionally, I developed a callback system to overcome the limitations of GLFW. Implementation details are provided in Appendix \ref{Input Controls}.

\section{UI}

I developed the application's UI using an object-oriented approach and a hierarchical structure, incorporating the Composite Design Pattern. Additionally, I implemented a save and load system for both UI parameters and the layout. Implementation details are available in Appendix \ref{UI}.

\newpage
\section{Repository Overview}

% Figure \ref{repo overview} presents the high-level structure of the project's code repository, showing the organization of key directories and files. Header files (.h) are omitted for clarity. All code was written from scratch, except for the use of third-party libraries.

% \renewcommand{\arraystretch}{1.5}

% \begin{table}[h]
% \centering
% \begin{longtable}{p{4cm} p{12cm}}
% \hline
% \textbf{Folder}        & \textbf{Description} \\
% \hline
% \texttt{Source/Input}           & Manages camera controls and callbacks. \\
% % \hline
% \texttt{Source/UI/App}          & Contains the top-level UI class. \\
% % \hline
% \texttt{Source/UI/Panels}       & Contains classes for different UI panels. \\
% % \hline
% \texttt{Source/UI/Properties}   & Contains various UI properties. \\
% % \hline
% \texttt{Source/Pipeline}        & Contains the application entry point, handles window management, Vertex Array Object (VAO), Vertex Buffer Object (VBO), Element Buffer Object (EBO), and shader preprocessing and compilation. \\
% % \hline
% \texttt{Source/Shaders}         & Contains all GLSL shaders. \\
% \hline
% \end{longtable}
% \caption{Repository Overview.}
% \label{table:folder-structure}
% \end{table}

% \renewcommand{\arraystretch}{1.0}
\vspace{-1em}
\begin{figure}[H]
\dirtree{%
.1 /.
.2 Source/.
.3 Input/.
.4 Camera.cpp.\DTcomment{Encapsulates camera transforms and parameters}.
.4 CameraController.cpp.\DTcomment{Controls the camera}.
.4 CallbackManager.cpp.\DTcomment{Manages GLFW callbacks}.
.3 Pipeline/.
.4 Main.cpp.\DTcomment{Application entry point}.
.4 Window.cpp.\DTcomment{Encapsulates GLFW windows}.
.4 Shader.cpp.\DTcomment{Handles OpenGL shaders, includes preprocessing and compilation}.
.4 FBO.cpp.\DTcomment{Encapsulates OpenGL Framebuffer Objects}.
.4 VAO.cpp.\DTcomment{Encapsulates OpenGL Vertex Array Objects}.
.4 VBO.cpp.\DTcomment{Encapsulates OpenGL Vertex Buffer Objects}.
% .4 EBO.cpp.\DTcomment{Encapsulates OpenGL Element Buffer Objects}.
.4 FPSCounter.cpp.\DTcomment{Tracks FPS}.
.3 UI/.
.4 App/.\DTcomment{Top-level UI class}.
.4 Panels/.\DTcomment{UI panels classes}.
.4 Properties/.\DTcomment{UI properties classes}.
.3 Shaders/. 
.4 Main.vert/.\DTcomment{VS for the screen quad}.
.4 Main.frag/.\DTcomment{FS for the screen quad}.
.4 Debug.frag/.\DTcomment{Renders debug views}.
.4 Shading.frag/.\DTcomment{Handles lighting and shadows}.
.4 Raymarching.frag/.\DTcomment{Implements ray marching}.
.4 Clouds.frag/.\DTcomment{Clouds generation and volumetric rendering}.
.4 Trees.frag/.\DTcomment{Trees generation}.
.4 Planet.frag/.\DTcomment{Planet generation}.
.4 ValueNoise.frag/.\DTcomment{Implements value noise}.
.4 IntersectionDistanceError.frag/.\DTcomment{Calculates IDE incrementally}.
.4 HeightDifferenceError.frag/.\DTcomment{Calculates HDE}.
.4 fBm.frag/.\DTcomment{Implements of fBm}.
.4 Terrain.frag/.\DTcomment{Heightmap generation}.
.4 SphericalAtmosphere.frag/.\DTcomment{Atmosphere simulation for planets}.
.4 Atmosphere.frag/.\DTcomment{Atmosphere simulation}.
.4 Profiling.frag/.\DTcomment{Profiling macros}.
.4 TwoDSky.frag/.\DTcomment{2D sky and clouds}.
.4 Encoding.frag/.\DTcomment{Encoding between floats and vec4s}.
% .4 TerrainMaterial.frag/.\DTcomment{Terrain procedural coloring}.
.4 Water.frag/.\DTcomment{Water normal map}.
.4 Sun.frag/.\DTcomment{Calculates the sun direction and renders the sun disk}.
.4 SmoothStep.frag/.\DTcomment{Smoothstep functions}.
.4 Hash.frag/.\DTcomment{Hash functions}.
.4 Motion.frag/.\DTcomment{Manages animations.}.
.3 Evaluation/.\DTcomment{Python scripts and notebooks for evaluation}.
.2 CMakeLists.txt.\DTcomment{CMake script for compilation}.
.2 LICENSE.txt.\DTcomment{Project license}.
}
\caption{Repository overview. Header files (.h) are omitted for clarity. All code was written from scratch, except for the use of third-party libraries.}
\label{repo overview}
\end{figure}

% DONE
% \question{Shall I also list all the shader files?}
% \JM{Yes, that is most of what you have done and an interesting aspect of the repository structure.}
% \todo{proper repo overview table that includes shader files}