\label{sec:3}

The app's workflow, as shown in Figure \ref{pipeline}, consists of three main stages: user interface (UI), procedural generation, and rendering. Given the project's scope -- encompassing the procedural generation and rendering of multiple natural elements -- it is practical to discuss each element individually, treating UI as a distinct subject.

The chapter begins with an overview of the implementation, followed by detailed sections on each natural element: terrain, trees, water, clouds, and the atmosphere. Finally, it discusses input handling, the UI, and how the app was extended to project procedural terrain onto a sphere to generate planets.

\section{Implementation Overview}
\label{Implementation Overview}

This chapter outlines the core architecture of the app, focusing on the execution sequences of both the graphics pipeline and the fragment shader within that pipeline.

\subsection{Pipeline Execution Order}

\mywrapfigure{0.6}{pipeline_order}{r}
{Outlines the execution order of the application pipeline.}{13}

Central to the ray marching rendering approach is the use of a \textbf{screen quad}. In this project, since both geometry and rendering processes are encapsulated within the fragment shader, only a basic quad covering the entire screen is necessary. This screen quad acts as the canvas on which all scene elements are rendered.


The application pipeline in OpenGL, following the flowchart in Figure \ref{pipeline_order}, is implemented in C++ and structured as follows:


\begin{enumerate}
    \item \textbf{Initialization}: The setup involves creating a window using GLFW, initializing OpenGL, and preparing a Vertex Buffer Object (VBO) and an Element Buffer Object (EBO) for the screen quad. The screen quad vertices and indices are predefined constants.
    \item \textbf{Shader Compilation}: This step includes preprocessing includes and compiling shaders, complete with error reporting.
    \item \textbf{Render Loop}: The main loop involves rendering the screen quad, handling user inputs for interactive elements and camera movement, and sending parameters as uniforms to the GPU.
\end{enumerate}

% \todo{Make the diagram smaller and inline on the right}
% \JM{Make smaller, like fig 2.4}}

\subsection{Fragment Shader Execution Order}

It is worth noting that while procedural generation is a distinct stage in the conceptual workflow, in practice, it occurs \textbf{implicitly} within the implementation. The geometry is defined by implicit functions, which are in turn determined by various parameters. Once these parameters are set, the procedural generation phase is effectively complete. Therefore, the focus of the fragment shaderâ€™s execution order is on the rendering aspects of the scene.

For flat, infinite worlds, following the flowchart in Figure \ref{shader_order}, the fragment shader operates in this sequence:
\begin{enumerate}
    \item Compute camera and sun rays.
    \item Ray march and render terrain and trees.
    \item For pixels with an intersection with terrain or trees, determine the intersection with the water surface and render the water.
    \item For pixels without intersections, render the sun disk.
    \item Simulate atmospheric Rayleigh scattering for each pixel, resulting in atmospheric perspective effects for intersected pixels, and determining the sky color for pixels without intersections.
    \item Render clouds.
\end{enumerate}

\myfigure{0.8}{shader_order}{}
{Outlines the execution order of the fragment shader.}

Before exploring each natural element, it is important to first outline the universal setup processes, including the calculation of both the camera and sun rays.

\subsubsection{Calculating the Camera Ray}

For each pixel, the process to calculate the camera ray (defined in Section \ref{Rendering}) involves transforming screen coordinates to a normalized space, and then determining the corresponding direction in the world space.

\paragraph{Screen to normalized space}
To make the rendering resolution-independent, each pixel's screen coordinates are converted to a normalized space. Let $\mathbf{P}$ be the screen coordinates and $\mathbf{res}$ be the resolution. The normalized coordinates $\mathbf{N}$ are given by:

\begin{equation}
   \mathbf{N} = \frac{2 \cdot \mathbf{P} - \mathbf{res}}{\min(\mathbf{res}.x, \mathbf{res}.y)}
\end{equation}

\paragraph{World position calculation}
To find the world position for each pixel, the camera parameters are used. Let $\mathbf{C}$ be the camera position, $\mathbf{f}, \mathbf{r}, \mathbf{u}$ be the forward, right and up directions respectively, and $f$ be the focal length. Here, $f$ is the distance in world coordinates between the camera $\mathbf{C}$ and the projection plane, influencing the field of view. The world position $\mathbf{W}$ is computed as:

\begin{equation}
   \mathbf{W} = \mathbf{C} + \text{normalize}(\mathbf{f}) \cdot f + \mathbf{N}.x \cdot \text{normalize}(\mathbf{r}) + \mathbf{N}.y \cdot \text{normalize}(\mathbf{u})
\end{equation}

\paragraph{Deriving the camera ray}
The direction of the camera ray $\mathbf{r_c}$ is the normalized vector from the camera position to the world position of the pixel. It is expressed as:

\begin{equation}
   \mathbf{r_c} = \text{normalize}(\mathbf{W} - \mathbf{C})
\end{equation}

These steps establish the direction of each ray from the camera to the pixel in world space, setting the foundation for ray marching.

\subsubsection{Calculating the Sun Ray}
\label{Sun Ray}

In this project, the \textbf{sun ray}, $\mathbf{r_s}$, is the normalized vector from a point in the world, towards the sun. The sun is treated as a directional light source due to its immense distance from Earth, implying that the sun rays are parallel at every point.

The sun ray's direction is represented using spherical coordinates, which offer a convenient method to define the direction in 3D space using only two parameters: azimuth ($\phi$) and elevation ($\theta$). These parameters are adjustable in the UI, allowing run-time changes to the sun ray. $\mathbf{r_s}$ is computed as follows:

\begin{equation}
    \mathbf{r_s} = \text{normalize}\left( \begin{bmatrix} \sin(\theta) \cos(\phi) \\ \cos(\theta) \\ \sin(\theta) \sin(\phi) \end{bmatrix} \right)
\end{equation}

This vector is critical for lighting calculations in the rendering process.


\section{Terrain}
\label{Terrain}

\subsection{Procedural Generation}
\label{Terrain Procedural Generation}

\subsubsection{Implementing fBm}
\label{Implement fbm}

As discussed in Section \ref{FBM}, we can construct fBm using fractal noise. The function $\text{fBm}_k(\mathbf{x}, n)$ is evaluated as per the pseudocode outlined in Algorithm \ref{algo:fbm}. The specifics of the value noise implementation are detailed in Section \todo{REF Appendix}.

\begin{algorithm}
\caption{Compute fBm Value at a Point}
\label{algo:fbm}
\begin{algorithmic}[1]
\Function{fBm}{$\mathbf{x}, n, \text{initial\_params}, \text{adjustment\_factors}$}
    \State $\text{fBm\_value} \gets 0$
    \State $\text{layer\_params} \gets \text{initial\_params}$
    \For{$i = 0$ \textbf{to} $n-1$}
        \State $\text{fBm\_value} \gets \text{fBm\_value} + \Call{ComputeLayerValue}{\mathbf{x}, \text{layer\_params}}$
        \State $\text{layer\_params} \gets \Call{UpdateLayerParams}{\text{layer\_params}, \text{adjustment\_factors}}$
    \EndFor
    \State \Return $\text{fBm\_value}$
\EndFunction
\Statex
\Function{UpdateLayerParams}{$\text{current\_params}, \text{adjustment\_factors}$}
    \State $f_i, a_i, \mathbf{R}_i, \mathbf{o}_i \gets \text{current\_params}$
    \State $\alpha, \beta, \mathbf{R}, \mathbf{o} \gets \text{adjustment\_factors}$
    \State \Return $(f_i \cdot \alpha, a_i \cdot \beta, \mathbf{R} \mathbf{R}_i, \mathbf{o} + \mathbf{o}_i)$
\EndFunction
\Statex
\Function{ComputeLayerValue}{$x, \text{layer\_params}$}
    \State $f_i, a_i, \mathbf{R}_i, \mathbf{o}_i \gets \text{layer\_params}$
    \State $\text{transformed\_x} \gets f_i \cdot \mathbf{R}_i \textbf{x} + \mathbf{o}_i$
    \State \Return $a_i \cdot \Call{ValueNoise}{\text{transformed\_x}}$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Heightmap Generation}

To generate a heightmap (Section \ref{Heightmap}) for terrain, we utilize the previously constructed fBm (details in Section \ref{Implement fbm}). The heightmap, $H(x,z)$, is defined as:

\begin{equation}
    H(x,z) = v\cdot\text{fBm}_{2}(\frac{(x,z)}{h})
\end{equation}

where $v$ represents a vertical scaling factor, and $h$ is a horizontal scaling factor, aiding in controlling the terrain's scale dimensions. $\text{fBm}_{2}$ is implemented by Algorithm \ref{algo:fbm}. Parameters for the fBm are made adjustable in the application's UI, as shown in Figure \ref{combined_3D}.

\myfigure{0.6}{local_heightmap}{}
{The image on the right shows a heightmap generated from fBm. The image on the left shows the associated terrain.}

\paragraph{Limitation}
The generated terrain, despite being infinite, lacks \textbf{variety}, typically manifesting as uniformly mountainous or plateau-like landscapes, as shown in Figure \ref{local_heightmap}.


\subsubsection{Dual Heightmap}
\label{Dual Heightmap}

To make the terrain more varied, I introduce a dual heightmap system:

\begin{enumerate}
    \item \textbf{Local Heightmap} ($H_{l}$): Utilizes fBm with higher frequencies to detail terrain on a smaller scale.
    \item \textbf{Global Heightmap} ($H_{g}$): Utilizes fBm with lower frequencies to define the overarching terrain shape and the extent of rockiness.
\end{enumerate}

The combined heightmap is formulated as follows:

\begin{equation}
H(x,z) = H_{g}(x,z) + \text{normalize}(H_{g}(x,z))\times H_{l}(x,z)
\end{equation}

This equation ensures:

\begin{itemize}
    \item The \textbf{addition} of $H_{g}(x,z)$ establishes the general terrain contour.
    \item The \textbf{multiplication} by $\text{normalize}(H_{g}(x,z))$ modulates the impact of $H_{l}(x,z)$, influencing the terrain's rockiness or ruggedness. Lower values of $H_{g}$ lead to gentler terrain, while higher values induce more pronounced elevation changes, akin to mountainous or rocky landscapes.
\end{itemize}

In summary, the dual heightmap approach effectively blends local details with global terrain contours to create diverse and realistic landscapes, as clearly depicted in the Figure \ref{combined_heightmap} and \ref{combined_3D} for comparison.

\myfigure{0.6}{combined_heightmap}{}
{contrasts slices of the local heightmap with the combined heightmap along the x-axis. It illustrates that the local heightmap, when used alone, results in repetitive terrain shapes. In contrast, the combined heightmap presents varied landscapes, featuring both mountainous and flatter regions. It also demonstrates how the global heightmap (the blue curve) predominantly shapes the general terrain features.}

\myfigure{1.0}{combined_3D}{h!}
{The first image shows a mountainous terrain from the local heightmap. Terrain in the second image combines local and global heightmaps, featuring both mountains and plateaus. The third image displays the application's UI with adjustable terrain generation parameters.}


\subsubsection{Domain Distortion}
\label{Domain Distortion}

To create terrain with natural variation, my project applies \textbf{domain distortion} to the heightmap. Domain distortion operates by modifying the input domain of a function before evaluation. For a given function $f(x)$, the domain is first distorted by a function $g(x)$, yielding $f(g(x))$ instead of the $f(x)$.

For the heightmap of the terrain, such distortion must be spatially varied to produce interesting results and continuous to avoid abrupt changes. This is achieved by incorporating a secondary 2D fBm, $\text{fBm}^\prime_2$, leading to the modified heightmap:

\begin{equation}
    H_l(x,z) = v\cdot\text{fBm}_{2}\left(\frac{x,z}{h}+\gamma\cdot\text{fBm}^\prime_2(x,z)\right)
\end{equation}

where $\gamma$ is the distortion magnitude, and $\text{fBm}^\prime_2$ utilizes distinct parameters. They are all adjustable via the applicationâ€™s UI.

This approach is grounded in the principle that natural landscapes are sculpted by processes such as erosion, sedimentary deposition, and tectonic activities. Domain distortion through $\text{fBm}^\prime_2$ gives the generated terrain variations that mimic these geological phenomena. The use of fBm is particularly advantageous as it layers noises of varying frequencies and amplitudes; lower frequencies mimic large-scale geological shifts, while higher frequencies emulate smaller-scale details like erosion. This multi-scale noise application is demonstrated in Figure \ref{distortion_scale}.

Choosing suitable parameter values for domain distortion is key. Figure \ref{distortion_heightmap} displays how varying the distortion magnitude, $\gamma$, impacts the heightmap. While fine-tuning these parameters encourages the emergence of natural-looking terrains, pushing the parameters beyond typical ranges can yield intriguing, alien landscapes, as depicted in Figure \ref{alien}.

\myfigurerow{0.50}{distortion_scale}{h!}
{Lower frequencies mimic large-scale geological shifts, while higher frequencies emulate smaller-scale details like erosion.}
{0.45}{alien}
{Exotic Terrain Features Generated by Domain Distortion}

\myfigure{1.0}{distortion_heightmap}{h!}
{Impact of Varying Distortion Magnitude $\gamma$ on a Heightmap}


\subsection{Ray Marching}
\label{Terrain Raymarching}

In the ray marching process for terrain rendering, I have implemented improvements on the standard fixed-step ray marching technique, as discussed in Section \ref{Raymarching Heightmaps}. These enhancements include the incorporation of early termination and dynamic step sizing, leading to more efficient and accurate rendering.

\paragraph{Early termination}

By halting the ray marching process under certain conditions, computational load can be reduced. In my implementation, the process terminates if:

\begin{enumerate}
    \item The marched distance surpasses a predefined maximum distance, $d_{\text{max}}$.
    \item The number of steps exceeds the maximum allowed, denoted as $N$.
    \item If a sample point's height exceeds the maximum terrain height (including trees), $y_{\text{max}}$, and the camera ray is ascending ($\mathbf{r_c}.y > 0$), the ray marching process terminates. This is based on the understanding that no intersection will occur if the ray is moving away from the terrain.
\end{enumerate}

\paragraph{Dynamic step size}

The step size in ray marching is dynamically adjusted based on two factors:

\begin{enumerate}
    \item The step size linearly increases with the \textbf{distance} already marched, aligning with the Level of Detail (LOD) concept. Farther objects can have less detail without significantly impacting the visual quality. The step size for the $i$th step, $s_i$, is defined as:

   \begin{equation}
   s_{i} = a + b \cdot d_i
   \end{equation}
   
   where $a$ and $b$ are adjustable parameters, and $d_i$ is the distance marched to the $i$th point.

    \item The step size also scales with the \textbf{height} above the terrain. This heuristic allows faster traversal of areas far from the terrain. The modified step size is:
    \begin{equation}
    s_{i} = a + b \cdot d_{i} + c \cdot (\mathbf{P}_i.y - H(\mathbf{P}_i.xz))
    \end{equation}

\end{enumerate}

\mywrapfigure{0.6}{bs}{r}
{Comparing linear interpolation and binary search for terrain intersections. The right diagram shows a 2-step binary search, where $\mathbf{M}_1$ is below the terrain, leading to $\mathbf{M}_2$ being the midpoint between $\mathbf{P}_2$ and $\mathbf{M}_1$. }{}

The final stage in the ray marching process involves refining the intersection point for enhanced precision. Traditionally (Section \ref{Raymarching Heightmaps}), this is done through linear interpolation between the last two points, where  $\mathbf{B}$ is the point below the terrain and $\mathbf{A}$ is above it. In my approach, I have improved this process by employing a binary search method, allowing for a more precise intersection determination, as illustrated in Figure \ref{bs}.

% \begin{algorithm}
% \caption{Binary Search Refinement for Terrain Intersections}
% \label{label:bs}
% \begin{algorithmic}
% \Function{BinarySearchRefinement}{$\mathbf{B}$, $\mathbf{A}$}
%     \For{$i = 0$ to $K - 1$}
%         \State $\mathbf{M} \gets 0.5 \cdot (\mathbf{B} + \mathbf{A})$
%         \State $\Delta h \gets \mathbf{M}.y - H(\mathbf{M}.xz)$

%         \If{$\Delta h < 0$}
%             \State $\mathbf{B} \gets \mathbf{M}$
%         \Else
%             \State $\mathbf{A} \gets \mathbf{M}$
%         \EndIf

%         \If{$|\Delta h| < \varepsilon$}
%             \State \textbf{break}
%         \EndIf
%     \EndFor
% \EndFunction
% \end{algorithmic}
% \end{algorithm}


% \JM{The algorithm seems unnecessary here; it is quite simple and also dmescribed in the text.}
% \todo{remove algo}



\subsection{Procedural Coloring}
\label{Terrain Procedural Texturing}

In my project, the terrain is categorized into \textbf{four distinct elevation-based regions}: mud (below water level), sand, grass, and rock (illustrated in Figure \ref{4regions}).

\myfigure{0.7}{4regions}{h}
{Cross-sectional representation of terrain illustrating the four elevation-based regions: mud, sand, grass, and rock. Striped textures are applied to steep areas.}

\paragraph{Smooth Boundaries}
The elevation thresholds separating these regions are denoted as $h_{1} < h_{2} < h_{3}$. Rather than using rigid boundaries, each threshold incorporates a variation $\pm \delta_i$ to facilitate smooth transitions. This is achieved by using the $\text{smoothstep}$ function to interpolate between regions.

\paragraph{Dynamic Boundaries}
To enhance realism and avoid monotonous separation at fixed elevations, a 2D fBm function offsets these boundaries. Consequently, at any point $\mathbf{P}$, the modified boundary is expressed as:

\begin{equation}
    h_{i} + \text{fBm}_{2}(\mathbf{P}.xz) \pm \delta_i
\end{equation}

This dynamic boundary adjustment is demonstrated in action in Figure \ref{region_transition}.



\paragraph{The Grass Region}
In the grass region, coverage is determined by the slope's steepness. A predefined threshold controls where grass appears: grass is rendered only on surfaces where the normal's y-component is below the threshold, indicating flatter regions. Interpolation is used to ensure smooth transition. 

Variation in the grass region is introduced by altering the coloration. By evaluating another 2D fBm on a point's xz coordinates, I smoothly transition between two distinct grass colors via interpolation. This method provides a more dynamic and visually appealing representation of grassy areas, as depicted in Figure \ref{grass}, which showcases the color-varied grass.

\myfigurerow{0.31}{region_transition}{}
{Demonstrates smooth and dynamic boundary adjustments between regions.}
{0.65}{grass}
{Aerial Perspective of the Color-Varied Grass Region. The left image displays the terrain with natural grass color variation, while the right image, in high-contrast black and white, emphasizes these variations for clearer visualization.}

\paragraph{Striped Rock Texture}
In steep areas, I have procedurally generated a rock texture resembling striped patterns found on mountains, as illustrated in Figure \ref{stripe}. This effect is created by horizontally stretching noise, producing stripe-like but non-uniform patterns. The degree of texture visibility is controlled by interpolating between the textured appearance and the rock's base color using the y-component of the normal. This ensures that stripes are prominent in steep regions, with a seamless transition to standard rock texture on flatter surfaces. Considering that the texture only appears on steep regions, I used the point's xy components as UV coordinates for texture sampling. 

\textbf{Importantly}, the texturing process does not involve creating and sampling from an image; rather, it queries a function directly.

\myfigure{0.8}{stripe}{h!}
{The left image displays the application of a striped pattern texture across the terrain. The right image illustrates the effect of interpolating this texture with the steepness of the terrain, resulting in the striped pattern showing only on steep slopes.}



\subsection{Illumination and Shadows}

\subsubsection{Shading}
\label{Terrain Shading}

In shading the terrain, I utilized the Phong reflection model \cite{phong_illumination_1975}, complemented by Fresnel Reflection (Section \ref{Fresnel}). The Phong model requires several inputs: the intersection point determined through ray marching (Section \ref{Terrain Raymarching}), the material color via procedural coloring (Section \ref{Terrain Procedural Texturing}), the incident light direction from the sun's spherical coordinates (Section \ref{Sun Ray}), and the terrain normal, analytically derived from the heightmap (outlined in Section \todo{REF}).

% I followed Equations \ref{ambient}, \ref{diffuse} and \ref{specular} to calculate the ambient, diffuse, and specular terms in the Phong model. The model's parameters are adjustable in the application's UI.

For added realism, the Phong shading model is augmented with Fresnel Reflection. Using Schlick's approximation (Equation \ref{schlick}) to compute the Fresnel term, $F(\theta)$, the specular term within the Phong model, $I_s$, is modified from Equation \ref{specular} to be:

\begin{equation}
I_{s} = k_s \cdot F(\theta) \cdot (\mathbf{r} \cdot \mathbf{-cr})^n \cdot I
\end{equation}

In my implementation, I bypassed the direct calculation of the incidence angle $\theta$ for Schlick's approximation; instead, I computed $\cos(\theta)$ directly using the negative dot product of the normal and the camera ray, $\mathbf{n} \cdot \mathbf{r_c}$.

\subsubsection{Shadows}
\label{Terrain Shadows}

\mywrapfigure{0.5}{shadow_terrain}{r}
{Illustration of soft shadow calculations for terrains. For the point in penumbra, its shadow ray doesn't intersect with the terrain, with $d_{\text{min}}$ incurred at $\mathbf{P}_2$.}{}

To make terrain cast soft shadows (as demonstrated in Figure \ref{terrain_soft}), I implemented a method for calculating the penumbra factor, $P$, based on the approach in Section \ref{Shadows}. Instead of measuring the actual distance from a shadow sample point to the terrain, I used vertical distance as a more efficient approximation. This method is visualized in Figure \ref{shadow_terrain} and implemented as outlined in Algorithm \ref{algo:soft shadow terrain}.

The shadow intensity, $S$, is subsequently integrated into the terrain's lighting model. The final color $\mathbf{c}$ of a fragment is calculated as:

\begin{equation}
    \mathbf{c} = S \cdot (I_{a} + I_{d} + I_{s})
\end{equation}

Here, $I_{a}$, $I_{d}$, and $I_{s}$ are the ambient, diffuse, and specular contributions to the fragment's color.



\begin{algorithm}
\caption{Terrain Shadow Calculation}
\label{algo:soft shadow terrain}
\begin{algorithmic}
\Function{terrain\_shadow}{$\mathbf{X}, \mathbf{r_s}$}
    \State $d_{\text{min}} \gets \infty$
    \For{$i = 0$ to $s_{\text{max}}$}
        \State $\mathbf{P}_i \gets \mathbf{X} + i \cdot s \cdot \mathbf{r_s}$
        \State $d_i \gets \mathbf{P}_i.y - H(\mathbf{P}_i.xz)$
        \State $d_{\text{min}} \gets \min(d_{\text{min}}, \frac{d_i}{i \cdot s})$
    \EndFor
    \State \Return $\text{smoothstep}(0, 1, d_{\text{min}})$
\EndFunction
\end{algorithmic}
\end{algorithm}

\myfigure{1.0}{terrain_soft}{h!}
{Comparison of terrain rendering: no shadows, hard shadows, and soft shadows.}

\section{Trees}

\subsection{Procedural Generation}
\label{Tree Procedural Generation}

In my project, each tree is modeled using the SDF(Section \ref{SDF}) of an ellipsoid, with added 3D fBm to mimic the foliage, and populated using domain repetition. 

\paragraph{SDF of Ellipsoids}
Instead of a simplistic approach scaling a sphere's SDF, a more accurate representation for ellipsoids is utilized (\todo{CITE}):

\begin{equation}
\text{SDF}_\text{ellipsoid}(\mathbf{X}, \mathbf{r}) = \frac{\left\|\frac{\mathbf{X}}{\mathbf{r}}\right\| \cdot \left(\left\|\frac{\mathbf{X}}{\mathbf{r}}\right\| - 1\right)}{\left\|\frac{\mathbf{X}}{\mathbf{r} \cdot \mathbf{r}}\right\|}
\end{equation}

Here, $\mathbf{r}$ is a vector specifying the ellipsoid's dimensions, and the origin is assumed to be the center of the ellipsoid.

\paragraph{fBm for Leaves}
The leaves are simulated by distorting the ellipsoid with a 3D fBm:

\begin{equation}
\text{SDF}_\text{tree}(\mathbf{X}) = \text{SDF}_\text{ellipsoid}(\mathbf{X}, \mathbf{r}) + \gamma \cdot \text{fBm}_3(\mathbf{X})
\end{equation}

Here, $\gamma$ represents the distortion magnitude, and it is adjustable in the application's UI along with the fBm parameters. The dimension $\mathbf{r}$ of the ellipsoids is randomized within a range to introduce size variation among trees.

\paragraph{Domain repetition for tree population}
To efficiently distribute trees across the terrain, a domain repetition technique is used. The SDF for a tree instance is made periodic on an infinite axis-aligned 2D square grid in the xz-plane. This results in a representation of infinite trees within this grid pattern. The tree positions are vertically adjusted by the terrain height and an additional constant $v$:

\begin{equation}
\begin{aligned}
\text{SDF}_\text{trees}(\mathbf{X}) &= \text{SDF}_\text{tree}(\mathbf{X} - \mathbf{C}), \\
\text{where } \mathbf{C}.x &= \mathbf{C}.z = \left(\left\lfloor \frac{\mathbf{x}}{d} \right\rfloor + 0.5\right) \cdot d, \\
\mathbf{C}.y &= H(\mathbf{X}.{xz}) + v.
\end{aligned}
\end{equation}

Here, $\mathbf{C}$ represents the center of the cell that $\mathbf{X}$ is in, and $d$ represents the cell size in the square grid. The domain repetition technique is depicted in Figure \ref{domain_rep}.

Moreover, trees are placed only on terrain that meets two criteria: the slope must be below a certain threshold to avoid cliffs, and the elevation must be higher than the water surface to prevent trees in water.

\paragraph{Randomization and Species Variation}
To avoid a uniform layout, a random offset $o(\mathbf{X})$, based on the position hashed, is introduced:

\begin{equation}
\begin{aligned}
\text{SDF}_\text{trees}(\mathbf{X}) &= \text{SDF}_\text{tree}(\mathbf{X} - \mathbf{C}), \\
\text{where } \mathbf{C}.x &= \mathbf{C}.z = \left(\left\lfloor \frac{\mathbf{X}}{d} \right\rfloor + 0.5\right) \cdot d + o(\mathbf{X}), \\
\mathbf{C}.y &= H(\mathbf{X}.{xz}) + v.
\end{aligned}
\end{equation}

Moreover, two species of trees (A and B) are introduced to add variety. The species type is determined using a 2D fBm; if $\text{fBm}(\mathbf{X})$ is below a certain threshold, the tree is of species A, otherwise species B. Each species has a different range for the randomized dimensions $\mathbf{r}$, allowing for distinct size characteristics between the species. The randomization of offsets and dimensions are depicted in Figure \ref{domain_rep}.

\paragraph{Considering Neighboring Cells}
The random offset $o(\mathbf{X})$ introduces complexity in determining the closest tree to any given point $\mathbf{X}$, as it may not necessarily be within the same cell as $\mathbf{X}$. To address this, I evaluate the surrounding eight cells, along with the cell containing $\mathbf{X}$, to find the nearest tree. The SDF for the trees, therefore, is calculated by finding the minimum distance to a tree among these nine cells:

\begin{equation}
\begin{aligned}
\text{SDF}_\text{trees}(\mathbf{X}) &= \min_{i\in\{-1,0,1\}}\min_{j\in\{-1,0,1\}}\text{SDF}_\text{tree}(\mathbf{X} - \mathbf{C_{ij}}), \\
\text{where } \mathbf{C_{ij}}.x & = \left(\left\lfloor \frac{\mathbf{X}}{d} \right\rfloor + i + 0.5\right) \cdot d + o(\mathbf{X}), \\
\mathbf{C_{ij}}.z & = \left(\left\lfloor \frac{\mathbf{X}}{d} \right\rfloor + j + 0.5\right) \cdot d + o(\mathbf{X}), \\
\mathbf{C_{ij}}.y &= H(\mathbf{X}.{xz}) + v.
\end{aligned}
\end{equation}

\myfigure{1.0}{domain_rep}{}
{Depicts domain repetition and the incorporation of randomized offsets and dimensions in tree generation, excluding species variation for simplicity.}

\subsection{Ray Marching}
\label{Tree Ray Marching}

While Sphere Tracing (Section \ref{Raymarching SDFs}) is a viable method for determining the intersection with the SDF of trees, initiating this process from the camera position for each tree intersection would be inefficient, considering that a similar ray marching process has just been done for the terrain.

To optimize efficiency, I integrated checks for intersections with tree canopies into the terrain ray marching algorithm. The tree canopy, defined by a constant height above the terrain, serves as an approximated bounding box for the trees, as depicted in Figure \ref{canopy}. Once the terrain intersection is identified through ray marching, the farthest canopy intersection point is then used as the starting position for Sphere Tracing of the trees. This approach substantially reduces sphere tracing steps, as illustrated and compared in Figure \ref{canopy}. 

\myfigure{1.0}{canopy}{}
{Illustrating the efficiency of my tree ray marching method: Sphere Tracing from the camera (left) takes 3 steps, while starting from the canopy intersection (right) requires only 1 step.}

\subsection{Procedural Coloring}

In the procedural coloring of trees, the primary objective is to introduce \textbf{color variation}, enhancing the realism of tree materials. 

I implemented two types of color variations: inter-species and intra-species. Each generated tree belongs to either species A or B (Section \ref{Tree Procedural Generation}), leading to the first type of variation, where different color schemes distinguish between the two species. Within each species, the second type of variation arises due to age differences among the trees. 

For any given pixel intersecting a tree in the rendered image, the process involves identifying the tree's species and then retrieving the corresponding color palette -- a 'young' color and an 'old' color for that species. To simulate intra-species variation, a random value between 0 and 1 is generated. This value is then used to interpolate between the young and old colors, resulting in a nuanced and realistic coloration for each tree, as demonstrated in Figure \ref{tree}.

\myfigure{0.8}{tree}{}
{The left image displays trees of one species in more saturated colors and another species in less saturated colors, with color variations indicating age differences. The right image uses higher contrast colors to differentiate the two species.}

\subsection{Illumination and Shadows}

\subsubsection{Shading}

For effective lighting of trees, determining the normal at the intersection point $\mathbf{P}$ is crucial. First, I obtain the normal of the tree's SDF using a numerical method (Section \todo{REF}). Subsequently, this SDF normal is blended with the normal of the underlying terrain at point $\mathbf{P}$. This blending facilitates the representation of tree lighting that conforms to the shape of the terrain beneath, mathematically expressed as:

\begin{equation}
    \mathbf{n}(\mathbf{P}) = a \cdot \mathbf{n}_{\text{SDF}}(\mathbf{P}) + (1 - a) \cdot \mathbf{n}_H(\mathbf{P}.xz)
\end{equation}

Here, $\mathbf{n}_{\text{SDF}}(\mathbf{P})$ is the SDF normal at point $\mathbf{P}$, $\mathbf{n}_H$ is the normal of the underlying terrain, and $a$ is a blending factor.

Similar to the shading approach for terrain (Section \ref{Terrain Shading}), the Phong Model with Fresnel reflection is employed for shading trees. Additionally, to approximate self-occlusion and ambient occlusion by nearby trees, I introduce an occlusion term $O$, computed as:

\begin{equation}
O = \text{smoothstep}(a, b, h_t)
\end{equation}

Here, $O$ is interpolated between 0 and 1 based on the height of the tree $h_t$, with parameters $a$ and $b$ controlling the interpolation. The diffuse component $I_d$ of the lighting model is then multiplied with $O$, resulting in lower parts of the tree appearing less lit than the upper parts, as demonstrated in Figure \ref{tree_illumination}.

\subsubsection{Shadows}
\label{Tree Shadows}

To make trees cast soft shadows, I adapted the technique used for terrains (see Section \ref{Terrain Shadows}), with adjustments to account for the translucent nature of tree foliage. Unlike solid terrain, tree foliage does not completely block light. I approximate the extent of shadowing with the degree of penetration of the shadow ray into the tree's volume. This is based on the principle that deeper penetration into the tree, likely indicating proximity to the trunk, results in deeper shadows.

To achieve this effect, following Algorithm \ref{algo: tree shadow}, I accumulate the value $(\text{SDF}_{\text{trees}}(\mathbf{P_s}) - \lambda) \cdot t$ along the shadow ray path. This accumulation, represented by $D$, is used to determine the shadow intensity. Here, $\mathbf{P_s}$ represents points along the shadow ray, $t$ is the distance of the ray, and $\lambda$ serves as a threshold for considering tree SDF in shadow calculations. The resultant accumulated value, $D$, is then interpolated using a smoothstep function to calculate the shadow intensity, $S$. If $D \le D_{\text{min}}$, the point is classified as being in umbra, fully shadowed. The result is demonstrated in Figure \ref{tree_illumination}.

\begin{algorithm}
\caption{Tree Shadow Calculation}
\label{algo: tree shadow}
\begin{algorithmic}
\Function{TreeShadow}{$\mathbf{P}$}
    \State $D \gets 0$
    \For{$t = 0$ to $t_{\text{max}}$ step $s$}
        \State $\mathbf{P}_s \gets \mathbf{P} + t \cdot \mathbf{r_s}$
        \State $d \gets \text{tree\_sdf}(\mathbf{P}_s)$
        \If{$d < \lambda$}
            \State $D \gets D + (d - \lambda) \cdot t$
        \EndIf
        \If{$D \leq D_{\text{min}}$}
            \State \textbf{break}
        \EndIf
    \EndFor
    \State $S \gets \text{smoothstep}(D_{\text{min}}, 0, D)$
    \State \Return $S$
\EndFunction
\end{algorithmic}
\end{algorithm}

\myfigure{1.0}{tree_illumination}{}
{Illustrating the complete process of tree illumination and shadowing.}

\section{Water}

\subsection{Representation}

In this project, a simple model was utilized to represent water. Assuming a constant water level globally, the water surface is modeled as a plane perpendicular to the y-axis. The elevation of this surface, denoted as $w$, serves as its defining parameter and can be adjusted in the UI. Instead of geometrically distorting the water surface to simulate waves, normal mapping was applied to mimic wave effects, as detailed in Section \ref{Water Shading}.

\subsection{Finding Intersection}

In the shader execution order, the intersection with the water is determined \textbf{after} the identification of terrain and tree intersections, as illustrated in the flowchart in Figure \ref{shader_order}. This order is chosen because water is transparent, and its visibility depends on whether it is occluded by these elements.

The visibility test checks if the camera ray intersects with the water surface before any terrain or tree. If the intersection point with either a tree or terrain is beneath the water level $w$, the water is considered visible.

To calculate the intersection with the water surface, an analytical approach is used. The intersection point $\mathbf{P}_w$ and distance $d_w$ from the camera to the water surface are calculated by:

\begin{equation}
d_{w} = \frac{w - \mathbf{C}.y}{\mathbf{r_c}.y}
\end{equation}

and

\begin{equation}
\mathbf{P}_{w} = \mathbf{C} + d_{w}\cdot\mathbf{r_c}
\end{equation}

where $\mathbf{C}$ denotes the camera position, $\mathbf{r_c}$ the normalized direction of the camera ray, and $w$ the water level.

\subsection{Procedural Coloring}

The procedural coloring of water focuses on two visual aspects: depth-dependent color variation and transparency. 

\paragraph{Depth-Based Color Variation}
   
The effective length of water that the camera ray travels through, denoted as $l$, is computed as the difference between the total distance to an object $d_{o}$ and the distance to the water surface $d_{w}$:

\begin{equation}
   l = d_{o}-d_w
\end{equation}

The \textbf{transmittance} of water, $\tau$, quantifies the amount of light that passes through the water. It decreases exponentially with the increase in $l$, modulated by a decay factor $k$:

\begin{equation}
    \tau = \exp(-k \times l)
\end{equation}
   
Water color is then interpolated between deep and shallow hues based on this transmittance $\tau$, emulating how water absorbs and scatters light differently at varying depths.

\paragraph{Transparency Effect}

Transparency is achieved by blending the water material color with the color at the underwater terrain intersection, based on the transmittance $\tau$. A higher $\tau$ indicates more light scatter, resulting in less light penetration and decreased transparency, and vice versa.

\subsection{Illumination}
\label{Water Shading}

A heightmap $H(x,z)$, constructed from a 2D fBm, represents water surface waves. The normal of this heightmap, $N_H$, serves as the normal map. The normal vector $\mathbf{n}$ at a point $\mathbf{X}$ on the water surface is:

\begin{equation}
\mathbf{n}= N_{H}(\mathbf{X}.xz)
\end{equation}

Details on calculating heightmap normals are in Section \todo{REF}.

Shading follows similar techniques as for terrain, utilizing the Phong model with a Fresnel effect, as detailed in Section \ref{Terrain Shading}. 

Reflections, although not implemented, could be approximated by rendering from the intersection point, using the reflected ray as the camera ray, limiting recursive renders, and weighting the reflection results progressively less. \question{Shall I talk about how reflections could be done in Conclusion: Future Extensions?}

\section{Atmosphere}
\label{Atmosphere}

This section outlines the physics-based rendering technique implemented for simulating atmospheric effects, primarily focusing on Rayleigh scattering. This phenomenon, caused by the interaction of sunlight with minuscule atmospheric particles, is crucial for achieving authentic sky colors and atmospheric perspective, as demonstrated in renders in Figure \ref{atmosphere}  and \ref{atmospheric} respectively.

The simulation examines the path of light from a point $\mathbf{E}$ in the scene to the camera at position $\mathbf{C}$, counter to the direction of the camera ray $\mathbf{r_c}$. It accounts for two key phenomena, as illustrated in Figure \ref{scatter}:
\begin{itemize}
    \item \textbf{In-scattering}: Light scatters from other directions into the direction of the ray.
    \item \textbf{Out-scattering}: The loss of light from the ray as it scatters in other directions.
\end{itemize}

\myfigure{0.7}{scatter}{}
{Highlights the in-scattering and out-scattering effects as the light travels through the atmosphere from point $\mathbf{E}$ to the camera at $\mathbf{C}$.}


\subsection{Modeling the Atmosphere}

Since the procedurally generated world is infinite in the xz direction, the atmospheric density depends solely on altitude. The atmosphere is given an upper limit $h_{\text{max}}$, above which the density is zero. The atmospheric density $D$ at a point $\mathbf{P}$ below the upper bound is modeled as:

\begin{equation}
    D(\mathbf{P})=\frac{e^{-\lambda \mathbf{P}.y}\left(h_{\text{max}}-\mathbf{P}.y\right)}{h_{\text{max}}}
\end{equation}

where $\lambda$ is a decay constant modifiable in the UI.

\subsection{The In-Scattering Equation}
\label{The In-Scattering Equation}

The in-scattering equation models the accumulation of sunlight along a ray from point $\mathbf{E}$ to the camera $\mathbf{C}$ through the atmosphere. It encompasses both in-scattering and out-scattering. Figure \ref{inscatter} provides a visual aid for understanding the related notations.

The equation is expressed as:

\begin{equation}
    I_v(\lambda) = I(\lambda) \times K(\lambda) \times F(\theta, g) \times \int_{\mathbf{C}}^{\mathbf{E}} \left( D(\mathbf{P}) \times \exp\left(-t\left({{\mathbf{PS}},\lambda}\right) -t \left({{\mathbf{PC}},\lambda}\right) \right)\right) ds
\end{equation}

where:
\begin{itemize}
    \item $I(\lambda)$ denotes the intensity of sunlight at wavelength $\lambda$.
    \item $K(\lambda)$ is the wavelength-dependent scattering coefficient. In the implementation, the coefficients for the red, green, and blue wavelengths are used, with their values being adjustable in the UI.
    \item $F(\theta, g)$ is the phase function, representing the angular distribution of scattering.
\end{itemize}

The end point $\mathbf{E}$ varies based on scene interaction:
\begin{itemize}
    \item If the camera ray intersects with an object (terrain, tree, or water), $\mathbf{E}$ is the intersection point, as shown in the right diagram of Figure \ref{inscatter}.
    \item If there's no intersection, $\mathbf{E}$ is the point where the camera ray $\mathbf{r_c}$ intersects the upper boundary of the atmosphere, as shown in the left diagram of Figure \ref{inscatter}.
\end{itemize}

In the integral:
\begin{itemize}
    \item $\mathbf{P}$ is a point on the segment between $\mathbf{C}$ and $\mathbf{E}$. $P = \mathbf{R}(s) = \mathbf{C} + s \cdot \mathbf{r_c}$, where $\mathbf{R}$ is the camera ray.
    \item $t$ is the out-scattering function.
    \item $\mathbf{S}$ is the intersection between the atmosphere's upper boundary with the ray with origin $\mathbf{P}$ and direction $\mathbf{r_s}$, as depicted in Figure \ref{inscatter}.
    \item $H_0$ is a parameter adjustable in the UI.
\end{itemize}

\myfigure{1.0}{inscatter}{}
{Visually clarify the notations and concepts related to the in-scattering equation. The left diagram represents a situation where the camera ray does not intersect any scene elements, and the right diagram shows a scenario with an intersection.}

The integral is approximated using Riemann sums, a method of summing rectangular areas to approximate the area under a curve. This technique involves marching along the camera ray, sampling at points $\mathbf{P}_i$ and evaluating the integrand at each point, then multiplying by the step size $s$. The sum of these calculations approximates the integral in the scattering equation. This process is illustrated in Figure \ref{riemann}.

\myfigure{1.0}{riemann}{}
{Visually demonstrates the Riemann sums method for the in-scattering equation: the left diagram represents a situation where the camera ray does not intersect any scene elements, and the right diagram shows a scenario with an intersection.}

\subsubsection{The Out-Scattering Function}

The out-scattering function $t$ describes the reduction of light as it traverses through the atmosphere and is given by:

\begin{equation}
    t(\mathbf{A}, \mathbf{B}, \lambda) = 4\pi \times K(\lambda) \times \int_{\mathbf{A}}^{\mathbf{B}} D(\mathbf{P}) ds
\end{equation}

\paragraph{Optical Depth}
\label{Optical Depth}

The integral represents the optical depth, essentially the atmospheric density averaged along the path segment multiplied by the length of the segment. This value indicates the cumulative effect of atmospheric particles on light along the ray path: more particles imply greater scattering of light away from the ray.

To approximate the optical depth, the Riemann sum method was employed, similar to the approach outlined in Section \ref{The In-Scattering Equation}.

\paragraph{The Phase Function}

The phase function $F$, essential in determining the directionality of scattering, is adapted from the Henyey-Greenstein function (\todo{CITE}):

\begin{equation}
    F(\theta, g) = \frac{3 \times (1 - g^2)}{2 \times (2 + g^2)} \times \frac{1 + \cos^2 \theta}{(1 + g^2 - 2 \times g \times \cos \theta)^{\frac{3}{2}}}
\end{equation}

Here, $\theta$ is the angle between the incoming light and the scattering direction. The asymmetry parameter $g$ influences the scattering distribution. For Rayleigh scattering, where $g = 0$, the function simplifies to $\frac{3}{4}(1 + \cos^2\theta)$, indicating more light scattered in directions close to the incoming light source.

\subsection{Blending}

The blending process involves combining the in-scattered light with light emitted at intersections in the scene. When the camera ray doesn't intersect any scene object, the sky color is determined solely by in-scattered light. However, at intersection points, the blending of in-scattered and emitted light is necessary, given by (\todo{CITE}):

\begin{equation}
    I'_v(\lambda) = I_v(\lambda) + I_e(\lambda)\exp(-t(\mathbf{C}, \mathbf{E}, \lambda))
\end{equation}

where $I_v(\lambda)$ represents the in-scattered light at wavelength $\lambda$, and $I_e(\lambda)$ is the light emitted at the intersection $\mathbf{E}$. 

For the final blended color $\mathbf{c}$ with RGB representation, I focus on three specific wavelengths corresponding to red, green, and blue light:

\begin{equation}
\mathbf{c} = (I_{v}'(\lambda_r), I_{v}'(\lambda_g), I_{v}'(\lambda_b))
\end{equation}

\myfigure{1.0}{atmosphere}{}
{Displays a series of screenshots with varying sun directions and altitudes: the first row captures lower altitudes, while the second showcases higher altitudes. Progressing from left to right, the sun's angle shifts from being more perpendicular to more oblique relative to the ground, mirroring the natural shift from midday to evening.}

\myfigure{0.7}{atmospheric}{}
{Compares renders before and after applying the atmosphere simulation, emphasizing the atmospheric perspective on the terrain. Note how the distant terrain appears more merged with the sky due to this effect.}

\section{Clouds}

\subsection{Procedural Generation of Clouds}

In modeling clouds, a \textbf{volumetric} approach is essential due to their complex, three-dimensional structure. Unlike terrain, clouds are not just surfaces; they are composed of particles with varying densities in three-dimensional space. To represent this, a density map, $D(\mathbf{X})$, is utilized, representing the density at any given point $\mathbf{X}$ in 3D space.

The procedural generation of this map begins with establishing a \textbf{bounding box} for the clouds, extending infinitely in the xz-plane and bounded vertically between lower $y_l$ and upper $y_u$ limits.

The generation involves two primary steps (as illustrated in Figure \ref{cloud_fbm}):
\begin{enumerate}
    \item The \textbf{base density} of the clouds is initially determined relative to their vertical position. The cloud density is intensified nearer to the box's central vertical axis, denoted by $y_c$, the mean of $y_u$ and $y_l$. The base density at a point $\mathbf{x}$ is formulated as:

\begin{equation}
    D(\mathbf{X}) = h - |\mathbf{X}.y - y_{c}|
\end{equation}

    where $h$ is a fraction of the total vertical extent of the cloud box.

    \item To this base density, a \textbf{3D fBm} is added, introducing natural, cloud-like irregularities and variations. The final density $D(\mathbf{x})$ at point $\mathbf{x}$ is then:

\begin{equation}
    D(\mathbf{X}) = h - |\mathbf{X}.y - y_{c}| + \text{fBm}_3(\mathbf{X})
\end{equation}
\end{enumerate}

In this approach, the combination of a base density gradient and the nature-simulating properties of fBm (Section \ref{FBM}) results in a realistic representation of cloud density. This method ensures clouds exhibit natural variations, while predominantly clustering towards the bounding box's center, avoiding unrealistic overflow.

\myfigure{0.8}{cloud_fbm}{}
{Illustrates the procedural generation process of the cloud density map. The diagram displays cross-sectional slices of the density map along the x-axis.}

\subsection{Volumetric Rendering}

Due to their transparent and volumetric properties, clouds cannot be rendered just using traditional ray marching to locate surface intersections. Instead, my approach involves sampling multiple points along the camera ray within the cloud volume. At each sampled point, color is calculated, and these colors are subsequently blended to form the final color. Itâ€™s important to note that this rendering technique is based on heuristic methods rather than a physically accurate simulation of light scattering phenomena, such as Mie Scattering.

\subsubsection{Volumetric Ray Marching}

The volumetric ray marching process involves calculating the start and end points of the ray within the cloud's bounding box and then marching the ray through the volume.

\paragraph{Start and end points}

The start $t_s$ and end $t_e$ distances along the ray from the camera position $\mathbf{C}$ are determined by intersecting the ray with the cloud's vertical boundaries, $y_l$ and $y_u$: 

\begin{equation}
    t_s = \max(\min(\frac{y_l - \mathbf{C}.y}{\mathbf{r_c}.y}, \frac{y_u - \mathbf{C}.y}{\mathbf{r_c}.y}), 0)
\end{equation}

\begin{equation}
    t_e = \min(\max(\frac{y_l - \mathbf{C}.y}{\mathbf{r_c}.y}, \frac{y_u - \mathbf{C}.y}{\mathbf{r_c}.y}), t_{\text{max}})
\end{equation}

Here $\mathbf{r_c}$ is the direction of camera ray, and $t_{\text{max}}$ is the maximum ray marching distance. The computations consider all the cases: where the camera position is below, in or above the bounding box.

\paragraph{Adaptive step size}

The step size $s$ is dynamically adjusted based on the cloud density $D(\mathbf{X})$ and the distance $t$ already covered by the ray. The density function $D(\mathbf{X})$ includes negative values, which, while uncharacteristic for physical densities, are utilized here for computational efficiency. These negative values indicate regions devoid of clouds, allowing the algorithm to increase the step size $s$ in proportion to the negativity of the density, thereby speeding up the march through empty spaces.

\begin{equation}
    s = a \times |D(\mathbf{X})| + s_{\text{min}}
\end{equation}

 Here, $a$ is a scaling factor for density, and $s_{\text{min}}$ is the minimum step size.

For positive density regions, the step size is also increased with distance $t$ to reduce computational load, as distant samples contribute less to visual detail and the final color blend.

\begin{equation}
     s = \max(s_{\text{min}}, b \times t)
\end{equation}

where $b$ is the distance scaling factor.

\paragraph{Termination criteria}

Ray marching is terminated under one of three conditions: 

\begin{enumerate}
    \item The ray exits the bounding box ($t\ge t_e$).
    \item A maximum number of steps is reached.
    \item The accumulated alpha value of the blended color reaches a threshold indicating sufficient opacity (as detailed in Section \ref{Cloud Blending}).
\end{enumerate}

\subsubsection{Illumination and Shadows}

Lighting is calculated at every sample point in the volumetric rendering process. The Phong model \cite{phong_illumination_1975} is adapted here by excluding the specular term, since the primary visual characteristics of particles in clouds are diffuse scattering and translucency. 

Shadows within the clouds, resulting from cloud particles obstructing light from other particles, are rendered using a technique called volumetric shadowing. This process involves marching a ray along the direction of the sun ray and accumulating the density of cloud particles at various points along this path. Higher densities indicate a greater number of particles blocking the light, resulting in deeper shadows at the sample point. Conversely, lower densities suggest fewer particles and less shadowing

\subsubsection{Blending}
\label{Cloud Blending}

The blending is done cumulatively, starting from the nearest point to the viewer and progressing towards the back. A cumulative color vector $\mathbf{c}$ (with RGBA components) is maintained and updated along the ray path.

For each sampled point $\mathbf{X_i}$, the alpha value $\alpha$ is calculated based on the density $D(\mathbf{X}_i)$ at that point, the step size $s$, and a scaling factor $\lambda$, with an upper limit set by the maximum cumulative alpha $\alpha_{\text{max}}$:

\begin{equation}
  \alpha = \text{clamp}(\lambda \times s \times D(\mathbf{X}_i), 0, \alpha_{\text{max}})
\end{equation}

The cumulative color $\mathbf{c}$ is then updated by blending with the color $\mathbf{c}_i$ at each sampled point $\mathbf{X_i}$:
  
\begin{equation}
  \mathbf{c} \mathrel{+}= \mathbf{c}_i \times \alpha \times (\alpha_{\text{max}} - \mathbf{c}.\alpha)
\end{equation}

Here, $\alpha$ prioritizes points with higher density, while the factor $(\alpha_{max} - \mathbf{c}.\alpha)$ prioritizes closer points. 

\todo{Final Blending with Color from previous steps}

\subsubsection{Sunlight Penetration Effect}

An additional effect emulates sunlight filtering through clouds, especially noticeable when the sun is behind them, as demonstrated in Figure \ref{clouds_sun}. This is achieved by augmenting $\mathbf{c}$ with the sun's color $\mathbf{c}_{\text{sun}}$, modulated by $\mathbf{c}.\alpha$, and the angle between the camera ray direction $\mathbf{r_c}$ and the sun direction $\mathbf{r_s}$:

\begin{equation}
  \mathbf{c}.rgb \mathrel{+}= a \cdot \mathbf{c}_{\text{sun}} \cdot (1 - b \cdot \mathbf{c}.\alpha) \cdot (\text{clamp}(\mathbf{r_c} \cdot \mathbf{r_s}, 0, 1))^ c
\end{equation}

Here, $a,b,c$ are parameters that are adjustable in the UI. $\mathbf{c}.\alpha$ indicates the cumulative cloud density along the camera ray, with a lower value indicating less cloud density and therefore more sunlight filtering through. Moreover, the effect is stronger when the camera ray is closely aligned with the sun ray, indicated by $\mathbf{r_c}\cdot \mathbf{r_s}$.

\myfigure{0.8}{clouds_sun}{}
{Comparison of Cloud Renders Demonstrating Sunlight Penetration Effect: The left image with the effect turned off, and the right image with the effect enabled.}

\section{Procedural Planets}

Since development was on schedule, I had time to implement an extension: applying the existing procedural generation and rendering of natural elements to planets. This section explains how I generate planet heightmaps, render the planet surface, and incorporate water and atmosphere.

\subsection{Heightmap Generation}

\minipagewrap{0.62}{planet_heightmap}{}
{Planet heightmap.}{
In my project, the heightmaps of planets differ from those of flat terrains. For a planet centered at point $\mathbf{O}$, given a direction $\mathbf{OP}$, the point $\mathbf{I}$ is where the ray in the direction of $\mathbf{OP}$ intersects the planet's surface. The heightmap function $H_{\text{planet}} (\mathbf{P})$ returns the length $\|\mathbf{OI}\|$, as illustrated in Figure \ref{planet_heightmap}.
}

To calculate $H_\text{planet} (\mathbf{P})$, I used triplanar projection (CITE) to project 3 heightmaps onto a sphere, computed as:

\begin{equation}
\begin{aligned}
H_\text{planet} (\mathbf{P}) = 
&H(\mathbf{P}.zy + f(\operatorname{sgn}(\mathbf{P}.x))) \times w_{x}+ \\
&H(\mathbf{P}.xz + f(\operatorname{sgn}(\mathbf{P}.y))) \times w_{y}+ \\
&H(\mathbf{P}.xy + f(\operatorname{sgn}(\mathbf{P}.z))) \times w_{z}
\end{aligned}
\end{equation}

Here, $H$ is the terrain heightmap (Section \ref{Terrain Procedural Generation}), the function $f$ ensures that opposite sides of the planet do not share the same elevation, and the weights $w_x,w_y,w_z$ are computed as follows:

\begin{equation}
\begin{aligned}
w_{x} &= \hat{|\mathbf{OP}}.x|^{k} \\
w_{y} &= \hat{|\mathbf{OP}}.y|^{k} \\
w_{z} &= \hat{|\mathbf{OP}}.z|^{k} \\
\label{tri weights}
\end{aligned}
\end{equation}

These weights are then normalized to sum up to 1. The parameter $k$ controls the blending sharpness between heightmaps, as illustrated in Figure \ref{k1}.


\mysubfigurerowthree{0.3}{k1}{h!}
{$k=1$}
{k10}{$k=10$}
{k100}{$k=100$}
{Comparison of triplanar-projected heightmaps with varying sharpness (\( k \)). Surface colors represent heightmap values, where black indicates high values and white indicates low values.}

\subsection{Ray Marching}

Similar to terrain ray marching (Section \ref{Terrain Raymarching}), I used fixed-step ray marching for the planet's heightmap, incorporating early termination and dynamic step sizes. As an additional optimization, if the camera is outside the planet's bounding sphere, I begin ray marching from the camera ray's intersection with this sphere.

\subsection{Procedural Coloring, Illumination and Shadows}

The procedural coloring, illumination, and shadow techniques are largely similar to those used for flat terrain (Section \ref{Terrain}), with a few notable differences.

Sinec the planet heightmap is a combination of three separate heightmaps, their normals need to be combined as well, a technique called triplanar normal mapping. First, the Whiteout Blend method (CITE) is used to blend the three normals along with the normal of the sphere at point $\mathbf{P}$, denoted as $\hat{\mathbf{OP}}$. The three normals are transformed from tangent space to world space and then blended using the weights for heightmap triplanar projection (Equation \ref{tri weights}). The code for this normal blending process is provided in Appendix \todo{REF}.

This approach ensures smooth and accurate normal calculations, as illustrated in Figure \ref{normal1}.

\mysubfigurerowthree{0.3}{normal1}{h!}
{Sphere normal.}
{normal2}{Planet normal.}
{normal3}{Shaded planet.}
{Visualizations of triplanar normal mapping.}

\subsection{Other Natural Elements}

\subsubsection{Water}

For planets, I model water surfaces as spheres rather than planes. To render water, I first calculate the intersection between the camera ray and the water surface (Appendix REF details how to find the intersection between a ray and a sphere). Procedural coloring (Section REF) and illumination (Section REF) are then applied similarly to how they are for water on flat terrain.

\subsubsection{Spherical Atmosphere}

The atmospheric density is still modeled as an exponential function based on altitude. However, the atmosphere is bounded by a sphere instead of a plane. For approximating the in-scattering equation, I only consider the segment of the ray that lies within the bounding sphere.

\subsection{Example Output}

Figure \ref{planet1} and \ref{planet4} display various planetary scenes rendered by my application. Refer to the accompanying video submission to see real-time navigation through these scenes.

\mysubfigurerowthree{0.3}{planet1}{h!}{}
{planet2}{}
{planet3}{}
{Example renders of an earth-like procedural planet in my application.}

\mysubfigurerowthree{0.3}{planet4}{h!}{}
{planet5}{}
{planet6}{}
{Example renders of various procedural planets in my application.}

\section{Input Controls}

\subsection{Camera Controls}

In this project, the scene features an infinitely expansive terrain, making traditional CAD camera controls like turn-table or trackball models unsuitable. A game-like camera control model was adopted for more intuitive navigation in such an extensive environment. The camera movement is controlled by the WASD and EQ keys, which allow movement along the three axes of the camera's coordinate system. The left mouse button drag controls the forward direction of the camera, while the scroll wheel adjusts the focal length. Right mouse dragging enables movement in the plane perpendicular to the forward direction. 

Moreover, I added trackball camera controls \cite{noauthor_object_nodate} for navigating planets in my planet extension (Section \todo{REF}), and included an option in the UI to switch control modes.

Camera parameters, including position and rotation, are first updated on the CPU using GLFW, and then passed as uniforms to the fragment shader. For detailed adjustments, the precise position and rotation can be fine-tuned through the UI.

\myfigure{0.4}{camera}{}
{This screenshot displays the camera panel in the UI, featuring adjustable settings and camera transforms. \todo{update and make inline}}

This control model enables efficient navigation through the extensive virtual world, proving especially useful for debugging and locating ideal camera angles for rendering.

\subsection{Callback System}

To address GLFW's limitation of handling only one callback per event, a centralized callback management system was implemented. This system stores and triggers user-defined callback functions for various input events. It activates the appropriate callbacks in response to specific events, thereby facilitating multiple reactions to a single input. This structure significantly enhances the flexibility and complexity of input management within the application.

\section{UI}

\subsection{Hierarchical Structure}

\mywrapfigure{0.4}{ui_hierarchy}{r}
{Outlines the hierarchical structure of the UI.}{}

The UI of the application was implemented following an Object-Oriented approach and a hierarchical structure, represented in Figure \ref{ui_hierarchy}.

The \textbf{App} is the singleton entity at the top level, containing multiple \textbf{panels}. Each panel inherits from a common abstract class, and comprises several \textbf{properties}. These properties are again derived from an abstract class and are responsible for managing individual or groups of parameters. The parameters correspond to uniforms on the GPU, facilitating real-time updates. UI interactions trigger updates to the corresponding uniforms in the GPU.

The hierarchy in implementation directly corresponds to the visual structure of the UI, as illustrated in Figure \ref{ui_visual}.

\myfigure{0.8}{ui_visual}{}
{Demonstrates the visual structure of the UI.}

\subsection{Composite Pattern}

In enhancing the UI's flexibility and organizational structure, the \textbf{Composite Design Pattern} plays a crucial role. This design pattern allows for treating individual objects and compositions of objects uniformly. In the context of the app's UI:

\begin{itemize}
    \item \textbf{GroupProperty}: Implemented as a single property, it is a collapsible group containing multiple properties.
    \item \textbf{TabPanel}: Implemented as a singular panel, it contains multiple sub-panels organized as tabs. 
\end{itemize}

Both elements allows for efficient space usage and better categorization, as demonstrated in Figure \ref{composite}.

\myfigure{0.6}{composite}{}
{Demonstrates GroupProperty and TabPanel. GroupProperty allows several properties to be organized into a collapsible group. TabPanel allows several panels to be organized as tabs in a single panel.}

\subsection{Save and Load}

The UI supports saving and loading of parameters and layout preferences.

\paragraph{Selection of JSON format}
I chose the JSON format for the app's saving and loading because of its compatibility with various data types, its hierarchical structure that mirrors the app's design, and its human readability.

\paragraph{Recursive Implementation}
The save and load functionality is implemented in a recursive manner. This design choice simplifies the process and ensures consistency across different levels of the UI. Each class in the application, including the app itself, every panel, and every property, implements $\text{save\_to\_json}$ and $\text{load\_from\_json}$ methods. This recursive approach mirrors the hierarchical nature of the UI, enabling seamless serialization and deserialization of parameters.

\paragraph{Scope of Save and Load}
One of the benefits of this hierarchical and recursive implementation is the flexibility it offers in terms of scope. It is possible to save and load configurations either globally or at the individual panel level. This flexibility is reflected in the UI, where buttons are provided in the menu bar for global operations, and within each panel for more localized control, as demonstrated in Figure \ref{save}.

\paragraph{Example JSON Structure}
A typical JSON file structure for saving the app's parameters resembles the following:

\begin{minted}{json}
{
    "Panel 1": {
        "Param X": {
            "value a": 10,
            "value b": 12
        },
        "Param Y": {
            "value": 1.2
        }
    },
    "Panel 2": {
        "Param Z": {
            "value": true
        }
    }
}
\end{minted}


\subsection{Layout Preferences}

\mywrapfigure{0.6}{save}{r}
{Demonstrates the UI of save and load functionalities.}{8}

In addition to parameter values, layout preferences such as the sizes and positions of panels are also managed through the save and load system. As shown in Figure \ref{save}, the menu bar includes options for saving and loading these layout configurations, further enhancing the appâ€™s user experience.


\section{Repository Overview}

Figure \ref{repo overview} presents the high-level structure of the project's code repository, showing the organization of key directories and files. Header files (.h) are omitted for clarity. All code was written from scratch, except for the use of third-party libraries.

% \renewcommand{\arraystretch}{1.5}

% \begin{table}[h]
% \centering
% \begin{longtable}{p{4cm} p{12cm}}
% \hline
% \textbf{Folder}        & \textbf{Description} \\
% \hline
% \texttt{Source/Input}           & Manages camera controls and callbacks. \\
% % \hline
% \texttt{Source/UI/App}          & Contains the top-level UI class. \\
% % \hline
% \texttt{Source/UI/Panels}       & Contains classes for different UI panels. \\
% % \hline
% \texttt{Source/UI/Properties}   & Contains various UI properties. \\
% % \hline
% \texttt{Source/Pipeline}        & Contains the application entry point, handles window management, Vertex Array Object (VAO), Vertex Buffer Object (VBO), Element Buffer Object (EBO), and shader preprocessing and compilation. \\
% % \hline
% \texttt{Source/Shaders}         & Contains all GLSL shaders. \\
% \hline
% \end{longtable}
% \caption{Repository Overview.}
% \label{table:folder-structure}
% \end{table}

% \renewcommand{\arraystretch}{1.0}

\begin{figure}
\dirtree{%
.1 /.
.2 Source/.
.3 Input/.
.4 Camera.cpp.\DTcomment{Encapsulates camera transforms and parameters}.
.4 CameraController.cpp.\DTcomment{Controls the camera}.
.4 CallbackManager.cpp.\DTcomment{Manages GLFW callbacks}.
.3 Pipeline/.
.4 Main.cpp.\DTcomment{Application entry point}.
.4 Window.cpp.\DTcomment{Encapsulates GLFW windows}.
.4 Shader.cpp.\DTcomment{Handles OpenGL shaders, includes preprocessing and compilation}.
.4 FBO.cpp.\DTcomment{Encapsulates OpenGL Framebuffer Objects}.
.4 VAO.cpp.\DTcomment{Encapsulates OpenGL Vertex Array Objects}.
.4 VBO.cpp.\DTcomment{Encapsulates OpenGL Vertex Buffer Objects}.
.4 EBO.cpp.\DTcomment{Encapsulates OpenGL Element Buffer Objects}.
.4 FPSCounter.cpp.\DTcomment{Tracks FPS}.
.3 UI/.
.4 App/.\DTcomment{Top-level UI class}.
.4 Panels/.\DTcomment{UI panels classes}.
.4 Properties/.\DTcomment{UI properties classes}.
.3 Shaders/. 
.4 Main.vert/.\DTcomment{VS for the screen quad}.
.4 Main.frag/.\DTcomment{FS for the screen quad}.
.4 Debug.frag/.\DTcomment{Renders debug views}.
.4 Shading.frag/.\DTcomment{Handles lighting and shadows}.
.4 Raymarching.frag/.\DTcomment{Implements ray marching}.
.4 Clouds.frag/.\DTcomment{Clouds generation and volumetric rendering}.
.4 Trees.frag/.\DTcomment{Trees generation}.
.4 Planet.frag/.\DTcomment{Planet generation}.
.4 ValueNoise.frag/.\DTcomment{Implements value noise}.
.4 IntersectionDistanceError.frag/.\DTcomment{Calculates IDE incrementally}.
.4 HeightDifferenceError.frag/.\DTcomment{Calculates HDE}.
.4 fBm.frag/.\DTcomment{Implements of fBm}.
.4 Terrain.frag/.\DTcomment{Heightmap generation}.
.4 SphericalAtmosphere.frag/.\DTcomment{Atmosphere simulation for planets}.
.4 Atmosphere.frag/.\DTcomment{Atmosphere simulation}.
.4 Profiling.frag/.\DTcomment{Profiling macros}.
.4 TwoDSky.frag/.\DTcomment{2D sky and clouds}.
.4 Encoding.frag/.\DTcomment{Encoding between floats and vec4s}.
.4 TerrainMaterial.frag/.\DTcomment{Terrain procedural coloring}.
.4 Water.frag/.\DTcomment{Water normal map}.
.4 Sun.frag/.\DTcomment{Calculates the sun direction and renders the sun disk}.
.4 SmoothStep.frag/.\DTcomment{Smoothstep functions}.
.4 Hash.frag/.\DTcomment{Hash functions}.
.4 Motion.frag/.\DTcomment{Manages animations.}.
.3 Evaluation/.\DTcomment{Python scripts and notebooks for evaluation}.
.2 CMakeLists.txt.\DTcomment{CMake script for compilation}.
.2 LICENSE.txt.\DTcomment{Project license}.
}
\caption{Repository overview}
\label{repo overview}
\end{figure}

% DONE
% \question{Shall I also list all the shader files?}
% \JM{Yes, that is most of what you have done and an interesting aspect of the repository structure.}
% \todo{proper repo overview table that includes shader files}